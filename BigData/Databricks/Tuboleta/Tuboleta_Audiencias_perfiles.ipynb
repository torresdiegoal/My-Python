{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33323da6-3bbf-4c72-8cdf-b6cc85f1ecaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74290994-411e-4f87-916c-d9b69558b7be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%pip install pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "443320d0-1658-4973-8bb0-d391e06af85d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,count,desc,sum,to_date,date_format,concat_ws,format_string,to_timestamp,year,when,lit,month\n",
    "from pyspark.sql import Window\n",
    "import pandas as pd\n",
    "\n",
    "# Crear una SparkSession\n",
    "# Crear una SparkSession\n",
    "spark = SparkSession.builder.appName(\"AzureSQL\").getOrCreate()\n",
    "\n",
    "## Credenciales BD\n",
    "inpath = \"C:/Users/diego.torres/OneDrive/Datasets/Tuboleta/Credenciales.txt\"\n",
    "keys = pd.read_csv(inpath, sep = ',')\n",
    "display(keys)\n",
    "\n",
    "# Creo variables para cada fila del DataFrame que contiene las credenciales de la bd\n",
    "for index, row in keys.iterrows():\n",
    "    variable_name = row['key']\n",
    "    variable_value = row['value']\n",
    "    globals()[variable_name] = variable_value\n",
    "\n",
    "# Configurar las propiedades de la conexión\n",
    "jdbc_url = f\"jdbc:sqlserver://{jdbc_hostname}:1433;database={jdbc_database_datamart}\"\n",
    "jdbc_properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": driver\n",
    "}\n",
    "\n",
    "# Conexion al Blob\n",
    "spark.conf.set(clave_blob, access_key_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f5a99a9-575c-47d6-a9cf-852a2f45e34c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## TABLA SALES\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/gold/Completo_ventas/Ds_G_sales.parquet'\n",
    "sales = spark.read.parquet(inpath)\n",
    "#print(sales.count())\n",
    "#display(sales.limit(100))\n",
    "\n",
    "sales.createOrReplaceTempView(\"sales_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7c9122-8e09-4380-a138-cba89200b820",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tmp = sales.select('SUB_TOPIC').distinct()\n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507697d9-f472-4771-83db-dd5501eadb37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tmp = sales.withColumn('TOWN_NW',col(\"TOWN\")).\\\n",
    "#     select('TOWN','SITE','TOWN_NW').distinct()\n",
    "# # #tmp = sales.filter(col('T_PRODUCT_ID') == '10229196910867') #eladio cali\n",
    "# # #print(tmp.count())\n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d323c52c-f754-47fa-8db6-47ceffb3ac20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# tmp = spark.sql(\n",
    "#     \"\"\"\n",
    "# SELECT DISTINCT\n",
    "#   TOPIC,\n",
    "#   TOPIC_N,\n",
    "#   TOPIC_PPTO\n",
    "# FROM sales_vw\n",
    "# WHERE YEAR(Fecha_evento) IN (2020)\n",
    "# \"\"\")\n",
    "\n",
    "# #print(tmp.count()) \n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff40989-e629-46b8-a32e-1190748ef515",
     "showTitle": true,
     "title": "FILE_NUMBER CON MAS DE UN PRODUCTO COMPRADO"
    }
   },
   "outputs": [],
   "source": [
    "# temp = sales.select('T_PRODUCT_ID','FILE_NUMBER').\\\n",
    "#     filter( col('product_family').isin('Paquete','Competición','Evento','Paquetes',\n",
    "# \t\t'Pases con fecha y hora','Pase de visita','Pases sin fecha')).distinct().\\\n",
    "#     orderBy(\"T_PRODUCT_ID\")\n",
    "\n",
    "# print(temp.count())\n",
    "\n",
    "# # Agregar una columna que indique si es duplicado o no\n",
    "# window_spec = Window().partitionBy(\"FILE_NUMBER\").orderBy(\"FILE_NUMBER\")\n",
    "\n",
    "# df_spark = temp.withColumn(\"es_duplicado\", count(\"FILE_NUMBER\").over(window_spec) > 1)\n",
    "\n",
    "# # Filtrar solo las filas duplicadas\n",
    "# duplicados = df_spark.filter(\"es_duplicado\")#.select('Evento').distinct()\n",
    "# #duplicados = duplicados.withColumn('Evento_N', lit('Hola'))\n",
    "# # DF = df3.toPandas()\n",
    "\n",
    "# # # Verificar duplicados en la columna 'llave_identificadora'\n",
    "# # duplicados = DF[DF.duplicated(subset=['Key_evento'], keep=False)]\n",
    "\n",
    "# #df_spark = spark.createDataFrame(duplicados)\n",
    "# display(duplicados)\n",
    "# print(duplicados.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e564a679-beef-42ee-917d-76b1afb3169b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# temp = sales.filter((col('FILE_NUMBER') == '9394807') #& col('T_OPERATION_TYPE').isin('SALE', 'REFUND')\n",
    "#                     ).\\\n",
    "#     select(['ORGANIZATION','FILE_NUMBER','T_PRODUCT_ID','PRODUCT','NET_SOLD_TKT_AMT_ITX','ORDER_NUMBER','T_OPERATION_TYPE','t_operation_kind']) \n",
    "# #temp = contacts.filter(col('CONTACT_NUMBER') == '9022570') #\n",
    "# display(temp.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "857f920e-29d5-4420-ace0-d6cbb46e3939",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# temp = sales.filter(col('t_operation_kind').isin('ORDER_OVERHEADS','DELIVERY_OVERHEADS')).\\\n",
    "#     select(['FILE_NUMBER','PRODUCT','NET_SOLD_TKT_AMT_ITX','ORDER_NUMBER','T_OPERATION_TYPE','t_operation_kind']).\\\n",
    "#     distinct() \n",
    "\n",
    "# # Agregar una columna que indique si es duplicado o no\n",
    "# window_spec = Window().partitionBy(\"FILE_NUMBER\").orderBy(\"FILE_NUMBER\")\n",
    "\n",
    "# df_spark = temp.withColumn(\"es_duplicado\", count(\"FILE_NUMBER\").over(window_spec) > 1)\n",
    "\n",
    "# # Filtrar solo las filas duplicadas\n",
    "# duplicados = df_spark.filter(\"es_duplicado\")#.select('Evento').distinct()\n",
    "\n",
    "# display(duplicados.limit(100))\n",
    "# #display(temp.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ce2b7d-e523-4cfb-83fe-d100c3b9cec8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7375106\n"
     ]
    }
   ],
   "source": [
    "eventos = spark.sql(\"\"\"\n",
    "    --el numero de eventos se contabiliza aparte debido a que para llegar a el, se deben realizar filtros distintos \n",
    "    --with num_eventos_ as (\n",
    "\tselect \n",
    "\t  s.T_ORDER_CONTACT_ID, \n",
    "\t  s.PRODUCT,\n",
    "\t  s.TEAM_1,\n",
    "\t  s.TEAM_2,\n",
    "\t  s.TOPIC,\n",
    "      s.SUB_TOPIC,\n",
    "      '' as PRODUCT_maestra,\n",
    "\t  s.TOPIC_N, \n",
    "\t  s.SUB_TOPIC_N,\n",
    "\t  s.PRODUCT_DATE_TIME,\n",
    "      sum(s.NET_SOLD_TKT_AMT_ITX) RECAUDO,\n",
    "      year(s.PRODUCT_DATE_TIME) as ANIO\n",
    "\tfrom sales_vw s\n",
    "\twhere product_family IN ('Competición','Evento','Pase de visita')\n",
    "\t\tAND UPPER(s.PRODUCT) NOT LIKE '%PRUEBA%'\n",
    "\t\tAND s.PRODUCT is not null\n",
    "\t\tAND T_ORDER_CONTACT_ID is not null\n",
    "\t\t--AND s.PRODUCT = 'DESDE QUE INVENTARON LAS EXCUSAS'\n",
    "  \t\t--AND UPPER(PRODUCT_STATE) != 'SUSPENDIDO'\n",
    "    group by T_ORDER_CONTACT_ID,\n",
    "    \ts.PRODUCT,\n",
    "     \tTEAM_1,TEAM_2,TOPIC,SUB_TOPIC,\n",
    "        TOPIC_N,SUB_TOPIC_N,\n",
    "        PRODUCT_DATE_TIME,ANIO\n",
    "\torder by T_ORDER_CONTACT_ID,\n",
    "   \t\tANIO,\n",
    " \t\ts.PRODUCT\n",
    "\t\"\"\")\n",
    "\n",
    "print(eventos.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9efa7e78-4286-4fba-9a41-43bdeb6fac5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #temp = eventos.select('PRODUCT','TOPIC','SUB_TOPIC','TOPIC_N','SUB_TOPIC_N','ANIO').distinct().filter(eventos.TOPIC_N == 'SIN DATO')\n",
    "# temp = eventos.select('PRODUCT','TOPIC','SUB_TOPIC','TOPIC_N','SUB_TOPIC_N','ANIO').distinct().filter(eventos.ANIO == '2024')\n",
    "# display(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009b2e48-c579-4f92-ba88-0303a3cca4e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(eventos.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48592f0-6a79-4198-a81e-c4eca7c673d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# resultado = eventos.groupBy(\"TOPIC_N\").count()\n",
    "# display(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f48063-191d-469e-8286-9fd5dc4b1baa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impresion\n",
    "# Guarda el DataFrame en formato Parquet en Azure Blob Storage\n",
    "inpath = \"abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/Eventos/\"\n",
    "# 2023_fcf\n",
    "join_df_coalesced = eventos.coalesce(1)\n",
    "join_df_coalesced.write.mode(\"overwrite\").parquet(inpath)\n",
    "#join_df_coalesced.write.mode(\"overwrite\").orc(path_parquet_snappy)\n",
    "\n",
    "#cambio de nombre\n",
    "files=dbutils.fs.ls(inpath)\n",
    "output_file= [x for x in files if x.name.startswith(\"part-\")]\n",
    "\n",
    "# EN la misma carpeta de origen\n",
    "dbutils.fs.mv(output_file[0].path, f\"{inpath}/Ds_G_Categoria_Eventos.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ace108-1964-46cd-8083-242cf0b8c612",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# # Obtener el año actual\n",
    "# anio_actual = datetime.now().year\n",
    "# anio_ant = anio_actual - 1\n",
    "\n",
    "# anio_actual = str(anio_actual)\n",
    "# anio_ant = str(anio_ant)\n",
    "# # Mostrar el año actual\n",
    "# #print(anio_actual,anio_ant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "255ff9c6-be60-4530-94fc-69359cecaa0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Lectura de bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d64efea-1375-41f8-8e9b-39fb96f9108e",
     "showTitle": true,
     "title": "Sales anterior-actual"
    }
   },
   "outputs": [],
   "source": [
    "# ## Ventas anio anterior\n",
    "# inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/sales/' + anio_ant + '/Ds_S3_Sales_' + anio_ant + '.parquet/'\n",
    "# sales_ant = spark.read.parquet(inpath)\n",
    "# print(sales_ant.count())\n",
    "\n",
    "# ## Ventas anio actual\n",
    "# inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/sales/' + anio_actual + '/S3/Ds_S3_Sales_' + anio_actual + '.parquet/'\n",
    "# sales_actual = spark.read.parquet(inpath)\n",
    "\n",
    "\n",
    "# ### 2023 ###\n",
    "# #Ds_S1_Sales_2023_11=Ds_B_Sales_2023_11_df.select(columnas)\n",
    "# sales_actual=sales_actual\n",
    "# print(sales_actual.count())\n",
    "\n",
    "# ### Union de Ventas anio anterior-actual\n",
    "# sales = sales_ant.union(sales_actual)\n",
    "# print(sales.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58bbfc0f-7e7b-4627-86a0-a98f7b13c20f",
     "showTitle": true,
     "title": "LOAD PAYMENTS"
    }
   },
   "outputs": [],
   "source": [
    "## Ventas 2019\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/payment/2019/Ds_S_PaymentMethods_2019.parquet/'\n",
    "payments_2019 = spark.read.parquet(inpath)\n",
    "#print(payments_2019.count())\n",
    "#payments_2019.createOrReplaceTempView(\"payments_2019_vw\")\n",
    "\n",
    "## payments_2020\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/payment/2020/Ds_S_PaymentMethods_2020.parquet/'\n",
    "payments_2020 = spark.read.parquet(inpath)\n",
    "#print(payments_2020.count())\n",
    "#payments_2020.createOrReplaceTempView(\"payments_2020_vw\")\n",
    "\n",
    "## payments_2021\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/payment/2021/Ds_S_PaymentMethods_2021.parquet/'\n",
    "payments_2021 = spark.read.parquet(inpath)\n",
    "#print(payments_2021.count())\n",
    "#payments_2021.createOrReplaceTempView(\"payments_2021_vw\")\n",
    "\n",
    "## payments_2022\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/payment/2022/Ds_S_PaymentMethods_2022.parquet/'\n",
    "payments_2022 = spark.read.parquet(inpath)\n",
    "#print(payments_2022.count()) \n",
    "#payments_2022.createOrReplaceTempView(\"payments_2022_vw\")\n",
    "\n",
    "## payments_2023\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/payment/2023/Ds_S_PaymentMethods_2023.parquet/'\n",
    "payments_2023 = spark.read.parquet(inpath)\n",
    "#print(payments_2023.count())\n",
    "#payments_2023.createOrReplaceTempView(\"payments_2023_vw\")\n",
    "\n",
    "\n",
    "## payments_2024\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/payment/2024/Ds_S_PaymentMethods_2024.parquet/'\n",
    "payments_2024 = spark.read.parquet(inpath)\n",
    "#print(payments_2024.count())\n",
    "#payments_2023.createOrReplaceTempView(\"payments_2023_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa7bd015-0df6-4bac-aa15-a521507cd9ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "payments = payments_2019.union(payments_2020).union(payments_2021).union(payments_2022).union(payments_2023).union(payments_2024)\n",
    "#sales = sales_2021.union(sales_2022).union(sales_2023)\n",
    "\n",
    "#print(sales_2023.count())\n",
    "payments.createOrReplaceTempView(\"payments_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b86147-30b9-4d92-b9b6-478f009ee1e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dim_payments = spark.sql(\"\"\"\n",
    "with temp1 as (\n",
    "\tselect \n",
    "\tFILE_NUMBER,\n",
    "\tPAYMENT_METHOD,\n",
    "\tsum(NET_SOLD_AMT_ITX) recaudo\n",
    "\tfrom payments_vw\n",
    "\tgroup by FILE_NUMBER,PAYMENT_METHOD)\n",
    "\n",
    "select distinct \n",
    "FILE_NUMBER,\n",
    "FIRST_VALUE(PAYMENT_METHOD) OVER (PARTITION BY FILE_NUMBER ORDER BY recaudo desc) AS PAYMENT_METHOD\n",
    "from temp1\n",
    "order by FILE_NUMBER\n",
    "\"\"\")\n",
    "\n",
    "# vista de dim_payments\n",
    "dim_payments.createOrReplaceTempView(\"dim_payments_vw\")\n",
    "#dim_payments.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93d3594-1a8b-4a4e-afa1-80cd19303466",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tmp = payments_2024.filter(month(col('REFERENCE_DATE')) == '3')\n",
    "# print(tmp.count())\n",
    "# tmp.createOrReplaceTempView(\"payments_2024_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36528b7e-6df0-4464-8634-ea7cb10aa2d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tmp = dim_payments.\\\n",
    "#     groupBy(\"PAYMENT_METHOD\").count().orderBy(col(\"count\").desc())\n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1db7e11b-77b0-4f74-a744-9bdc8bbb5ac0",
     "showTitle": true,
     "title": "Payments anterior-actual"
    }
   },
   "outputs": [],
   "source": [
    "# ## Payments anio anterior\n",
    "# inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/payment/' + anio_ant + '/Ds_S_PaymentMethods_' + anio_ant + '.parquet/'\n",
    "# payments_ant = spark.read.parquet(inpath)\n",
    "# print(payments_ant.count())\n",
    "\n",
    "# ## Payments anio actual\n",
    "# inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/payment/' + anio_actual + '/Ds_S_PaymentMethods_' + anio_actual + '.parquet/'\n",
    "# payments_actual = spark.read.parquet(inpath)\n",
    "# print(payments_actual.count())\n",
    "\n",
    "\n",
    "# ### Union de Ventas anio anterior-actual\n",
    "# payments = payments_ant.union(payments_actual)\n",
    "# print(payments.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c901ed8-f867-435e-ad2e-689109f52054",
     "showTitle": true,
     "title": "Contacts"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3844843\n"
     ]
    }
   ],
   "source": [
    "## Contacts\n",
    "inpath = \"abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/Contacts/Ds_S_Contacts.parquet\"\n",
    "contacts = spark.read.parquet(inpath)\n",
    "print(contacts.count())\n",
    "\n",
    "contacts.createOrReplaceTempView(\"contacts_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e66b590-2097-42bf-9a3f-768ddaa063d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #temp = contacts.filter(col('T_CONTACT_ID') == '10229522833023')\n",
    "# temp = contacts.filter(col('CONTACT_NUMBER') == '9033085')\n",
    "# display(temp.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f35420-966c-47f0-b849-b491b481c414",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# temp = contacts.select('T_CONTACT_ID').distinct()\n",
    "# print(temp.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbbf364-9d25-44dc-a0da-9ae27076d5c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Consulta de audiencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d2fe43-af69-4010-a208-c6039e195674",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tmp = dim_payments.filter(\"PAYMENT_METHOD\").count().orderBy(col(\"count\").desc())\n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca35cb9-be8e-489e-8720-4aff5078edb1",
     "showTitle": true,
     "title": "TABLA DE HISTORICO PARA CIENCIAS DE DATOS"
    }
   },
   "outputs": [],
   "source": [
    "historico = spark.sql(\"\"\"\n",
    "\n",
    "WITH tmp11 as (\n",
    "    SELECT DISTINCT\n",
    "    Key_evento,\n",
    "    Key_aforo,\n",
    "    Nuevo_aforo as Aforo\n",
    "    FROM sales_vw ),\n",
    " \n",
    " tmp12 as (\n",
    "\tSELECT \n",
    "    Key_evento,\n",
    "    SUM(Aforo) as Aforo\n",
    "\tFROM tmp11\n",
    "  GROUP BY Key_evento)\n",
    "\n",
    "SELECT\n",
    "s.FILE_NUMBER as Expediente,\n",
    "s.T_ORGANIZ_ID as Codigo_organismo,\n",
    "s.ORGANIZATION as Organismo,\n",
    "s.REFERENCE_DATE as Fecha_Compra,\n",
    "s.PRODUCT_DATE_TIME as Fecha_hora_evento,\n",
    "s.Fecha_evento,\n",
    "s.Hora_evento,\n",
    "s.Key_evento,\n",
    "c.Edad,\n",
    "c.Rango_de_edad,\n",
    "c.Genero, \n",
    "CASE \n",
    "    WHEN c.CIUDAD IS NULL THEN 'SIN DATO'\n",
    "    ELSE c.CIUDAD\n",
    "  END AS Ciudad_del_cliente,\n",
    "c.REGION as Region_del_cliente,\n",
    "c.DEPARTAMENTO as Departamento_del_cliente,\n",
    "c.CAPITAL as Ciudad_capital_del_cliente,\n",
    "s.SALES_CHANNEL_TYPE as Canal_de_venta,\n",
    "s.product_family as Familia_producto,\n",
    "s.T_PRODUCT_ID as Codigo_evento,\n",
    "s.PRODUCT_N as Evento,\n",
    "--s.PRODUCT_CODE,\n",
    "s.T_PERFORMANCE_ID as Sesion,\n",
    "s.TEAM_1,\n",
    "s.TEAM_2_N as TEAM_2,\n",
    "s.TOPIC as Tema,\n",
    "s.SUB_TOPIC as Subtema,\n",
    "s.TOWN_N as Ciudad_venue,\n",
    "s.SITE as Venue,\n",
    "--p.PAYMENT_METHOD as Metodo_de_pago,\n",
    "s.TOPIC_N as Tema_gral,\n",
    "s.SUB_TOPIC_N as Subtema_gral,\n",
    "s.Nuevo_aforo as Aforo_localidad,\n",
    "t.Aforo,\n",
    "s.LOGICAL_SEAT_CATEGORY,\n",
    "s.T_OPERATION_TYPE,\n",
    "s.T_OPERATION_KIND,\n",
    "SUM(s.NET_SOLD_T_QTY)- SUM(s.NET_SOLD_C_QTY) as Tickets_comprados,\n",
    "SUM(s.NET_SOLD_C_QTY) AS Cortesias,\n",
    "SUM(s.NET_SOLD_TKT_AMT_ITX) AS Recaudo\n",
    "FROM sales_vw s\n",
    "LEFT JOIN contacts_vw c\n",
    "  ON s.T_ORDER_CONTACT_ID = c.T_CONTACT_ID\n",
    "LEFT JOIN tmp12 t\n",
    "  ON s.Key_evento = t.Key_evento\n",
    "--LEFT JOIN dim_payments_vw p\n",
    "--  ON s.FILE_NUMBER = p.FILE_NUMBER\n",
    "--LEFT JOIN Maestra_subtopic_2_vw m\n",
    "--  ON s.PRODUCT = m.PRODUCT\n",
    "WHERE --YEAR(s.REFERENCE_DATE) >= 2022\n",
    "    s.T_OPERATION_TYPE IN ('SALE','REFUND')\n",
    "\t\tAND s.T_OPERATION_KIND IN ('SINGLE_ENTRY','PRODUCT_COMPOSITION','SIMPLE_PRODUCT')--'COMPOSED_PRODUCT' \n",
    "\t\tAND s.PRODUCT_CODE != 'CAN_INS'\n",
    "    --AND s.product_family IN ('Paquete','Competición','Evento','Paquetes',\n",
    "        --'Pases con fecha y hora','Pase de visita','Pases sin fecha') \n",
    "GROUP BY\n",
    "s.FILE_NUMBER,s.T_ORGANIZ_ID,s.ORGANIZATION,s.REFERENCE_DATE,s.PRODUCT_DATE_TIME,\n",
    "s.Fecha_evento,\n",
    "s.Hora_evento,\n",
    "s.Key_evento,c.Edad,c.Rango_de_edad,c.Genero,c.CIUDAD,c.REGION,c.DEPARTAMENTO,c.CAPITAL,s.SALES_CHANNEL_TYPE,s.product_family,\n",
    "s.T_PRODUCT_ID,s.PRODUCT_N,--s.PRODUCT_CODE,\n",
    "s.T_PERFORMANCE_ID,s.TEAM_1,s.TEAM_2_N,s.TOPIC,s.SUB_TOPIC,s.TOWN_N,s.SITE,--p.PAYMENT_METHOD,\n",
    "s.TOPIC_N,s.SUB_TOPIC_N,s.Nuevo_aforo,t.Aforo,s.DN_QUOTA,s.LOGICAL_SEAT_CATEGORY,s.T_OPERATION_TYPE,s.T_OPERATION_KIND\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "#historico_.count()\n",
    "# display(historico_.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68265d76-1eb6-491c-9492-553e0c4965b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impresion\n",
    "# Guarda el DataFrame en formato Parquet en Azure Blob Storage\n",
    "inpath = \"abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/Modelos_analitica/Base_historico/\"\n",
    "# 2023_fcf\n",
    "join_df_coalesced = historico.coalesce(1)\n",
    "join_df_coalesced.write.mode(\"overwrite\").parquet(inpath)\n",
    "\n",
    "#cambio de nombre\n",
    "files=dbutils.fs.ls(inpath)\n",
    "output_file= [x for x in files if x.name.startswith(\"part-\")]\n",
    "\n",
    "# EN la misma carpeta de origen\n",
    "dbutils.fs.mv(output_file[0].path, f\"{inpath}/Ds_G_Base_historico.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fee600e6-f12b-4541-a7d5-18f1ef9563bf",
     "showTitle": true,
     "title": "HANDLING FEE"
    }
   },
   "outputs": [],
   "source": [
    "# handling = spark.sql(\n",
    "#     \"\"\"\n",
    "# WITH temp as (\n",
    "#     SELECT\n",
    "#     s.FILE_NUMBER as Expediente,\n",
    "#     SUM(s.NET_SOLD_TKT_AMT_ITX) as Handling_fee\n",
    "#     FROM sales_vw s\n",
    "#     WHERE s.T_OPERATION_TYPE IN ('SALE','REFUND')\n",
    "#         AND s.T_OPERATION_KIND IN ('DELIVERY_OVERHEADS','ORDER_OVERHEADS')--'COMPOSED_PRODUCT' \n",
    "#     --\t\tAND s.PRODUCT_CODE != 'CAN_INS'\n",
    "#         --AND s.product_family IN ('Paquete','Competición','Evento','Paquetes',\n",
    "#             --'Pases con fecha y hora','Pase de visita','Pases sin fecha') \n",
    "#     GROUP BY\n",
    "#     s.FILE_NUMBER,s.REFERENCE_DATE\n",
    "# ),\n",
    "\n",
    "# WITH temp as ()\n",
    "\n",
    "# SELECT\n",
    "# CONCAT(Expediente,'|',Mes_anio) as Key_handling,\n",
    "# SUM(Handling_fee) Handling_fee\n",
    "# FROM temp\n",
    "# GROUP BY Expediente,Mes_anio\n",
    "# ORDER BY Expediente,Mes_anio\n",
    "# \"\"\"\n",
    "# )\n",
    "\n",
    "# print(handling.count())\n",
    "# display(handling.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "862f43ef-1ce6-498c-8e41-e1d6d8a6e064",
     "showTitle": true,
     "title": "AUDIENCIAS MERCADEO"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8889655"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audiencias_merch = spark.sql(\n",
    "    \"\"\"\n",
    "SELECT\n",
    "s.FILE_NUMBER,\n",
    "s.ORGANIZATION as Organismo,\n",
    "s.T_ORDER_CONTACT_ID as Id_contacto,\n",
    "c.FIRSTNAME as NOMBRE,\n",
    "c.LASTNAME as APELLIDO,\n",
    "s.ORDER_CONTACT_NUMBER as COD_CONTACTO,\n",
    "c.FECHA_DE_NACIMIENTO,\n",
    "s.REFERENCE_DATE as Fecha_Compra,\n",
    "s.PRODUCT_DATE_TIME as Fecha_hora_evento,\n",
    "s.Fecha_evento,\n",
    "--s.Hora_evento,\n",
    "s.Key_evento,\n",
    "s.Key_aforo,\n",
    "s.ORDER_CREATION_DATE AS Hora_de_compra,\n",
    "c.Edad,\n",
    "c.Rango_de_edad,\n",
    "c.Genero,\n",
    "c.EMAIL as Correo,\n",
    "c.NAT_NUMBER_CELLPHONE as Movil,\n",
    "c.MAIN_ADDR_LINE1 as Direccion, \n",
    "CASE \n",
    "    WHEN c.CIUDAD IS NULL THEN 'SIN DATO'\n",
    "    ELSE c.CIUDAD\n",
    "  END AS Ciudad_del_cliente,\n",
    "c.CAPITAL as Ciudad_capital_del_cliente,\n",
    "c.DEPARTAMENTO as Departamento_del_cliente,\n",
    "c.PAIS as Pais_del_cliente,\n",
    "s.SALES_CHANNEL as Canal_de_venta,\n",
    "s.SALES_CHANNEL_N,\n",
    "s.SALES_CHANNEL_TYPE as Subcanal,\n",
    "s.product_family as Familia_producto,\n",
    "s.T_PRODUCT_ID as Codigo_evento,\n",
    "s.PRODUCT_N as Evento,\n",
    "s.PRODUCT_CODE,\n",
    "s.LICENCE_NUMBER as Codigo_pulep,\n",
    "s.PROMOTION as Promocion,\n",
    "s.LOGICAL_SEAT_CATEGORY as Categoria_Logica,\n",
    "s.TOWN_N as Ciudad_venue,\n",
    "s.SITE as Venue,\n",
    "p.PAYMENT_METHOD as Metodo_de_pago,\n",
    "s.TOPIC_PPTO as Tema_gral,\n",
    "s.SUB_TOPIC_N as Subtema_gral,\n",
    "s.Nuevo_aforo as Aforo,\n",
    "s.PERFORMANCE_QUOTA as Aforo_old,\n",
    "s.T_OPERATION_TYPE,\n",
    "s.T_OPERATION_KIND,\n",
    "s.INVOICE_CONTACT_FMT_NAME as Promotor,\n",
    "SUM(s.NET_SOLD_T_QTY)- SUM(s.NET_SOLD_C_QTY) as Tickets_comprados,\n",
    "SUM(s.NET_SOLD_C_QTY) AS Cortesias,\n",
    "SUM(s.NET_SOLD_TKT_AMT_ITX) AS Recaudo\n",
    "FROM sales_vw s\n",
    "LEFT JOIN contacts_vw c\n",
    "  ON s.T_ORDER_CONTACT_ID = c.T_CONTACT_ID\n",
    "LEFT JOIN dim_payments_vw p\n",
    "  ON s.FILE_NUMBER = p.FILE_NUMBER\n",
    "--LEFT JOIN Maestra_subtopic_2_vw m\n",
    "--  ON s.PRODUCT = m.PRODUCT\n",
    "WHERE --YEAR(s.REFERENCE_DATE) >= 2022\n",
    "  s.T_OPERATION_TYPE IN ('SALE','REFUND')\n",
    "\tAND s.T_OPERATION_KIND IN ('SINGLE_ENTRY','PRODUCT_COMPOSITION','SIMPLE_PRODUCT')--'COMPOSED_PRODUCT' \n",
    "\tAND s.PRODUCT_CODE != 'CAN_INS'\n",
    "    --AND s.product_family IN ('Paquete','Competición','Evento','Paquetes',\n",
    "        --'Pases con fecha y hora','Pase de visita','Pases sin fecha') \n",
    "GROUP BY\n",
    "s.FILE_NUMBER,s.ORGANIZATION,s.T_ORDER_CONTACT_ID,c.FIRSTNAME,c.LASTNAME,s.ORDER_CONTACT_NUMBER,c.FECHA_DE_NACIMIENTO,s.REFERENCE_DATE,s.PRODUCT_DATE_TIME,s.Fecha_evento,--s.Hora_evento,\n",
    "s.Key_evento,s.Key_aforo,\n",
    "s.ORDER_CREATION_DATE,c.Edad,c.Rango_de_edad,c.Genero,c.EMAIL,c.NAT_NUMBER_CELLPHONE,c.ADDRESS_SALUTATION,c.MAIN_ADDR_LINE1,c.CIUDAD,c.CAPITAL,c.DEPARTAMENTO,c.PAIS,s.SALES_CHANNEL,s.SALES_CHANNEL_N,s.SALES_CHANNEL_TYPE,s.product_family,s.T_PRODUCT_ID,s.PRODUCT_N,s.PRODUCT_CODE,s.LICENCE_NUMBER,s.PROMOTION,s.LOGICAL_SEAT_CATEGORY,s.TOWN_N,s.SITE,p.PAYMENT_METHOD,s.TOPIC_PPTO,s.SUB_TOPIC_N,s.Nuevo_aforo,s.PERFORMANCE_QUOTA,s.T_OPERATION_TYPE,s.T_OPERATION_KIND,s.INVOICE_CONTACT_FMT_NAME\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "audiencias_merch.count()\n",
    "# display(audiencias_merch.limit(10))\n",
    "# print(audiencias_merch.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "670a59ab-2cbf-4813-be15-ff7485b34654",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "archivo_csv = \"abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/temp/metodo/metodos_pago_2.csv\"\n",
    "df = spark.read.csv(archivo_csv, header=True, inferSchema=True, sep=';',encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Renombra la columna en `df` para evitar conflictos de nombres\n",
    "df = df.withColumnRenamed(\"Metodo_de_pago\", \"Metodo_de_pago_2\")\n",
    "\n",
    "# Realiza un join basado en la columna \"Metodo_de_pago\" utilizando alias\n",
    "audiencias_merch = audiencias_merch.alias(\"a\").join(\n",
    "    df.alias(\"b\"),\n",
    "    col(\"a.Metodo_de_pago\") == col(\"b.Metodo_de_pago_2\"),\n",
    "    \"left_outer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "308c25a5-d02c-4a76-9659-7cab3381c121",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-591896717185790>, line 3\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col, initcap\n",
       "\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Obtiene todas las columnas\u001b[39;00m\n",
       "\u001b[0;32m----> 3\u001b[0m todas_las_columnas \u001b[38;5;241m=\u001b[39m audiencias_merch\u001b[38;5;241m.\u001b[39mcolumns\n",
       "\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Selecciona solo las columnas de tipo cadena\u001b[39;00m\n",
       "\u001b[1;32m      6\u001b[0m columnas_cadena \u001b[38;5;241m=\u001b[39m [col_name \u001b[38;5;28;01mfor\u001b[39;00m col_name, dtype \u001b[38;5;129;01min\u001b[39;00m audiencias_merch\u001b[38;5;241m.\u001b[39mdtypes \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
       "\n",
       "\u001b[0;31mNameError\u001b[0m: name 'audiencias_merch' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\nFile \u001b[0;32m<command-591896717185790>, line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col, initcap\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Obtiene todas las columnas\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m todas_las_columnas \u001b[38;5;241m=\u001b[39m audiencias_merch\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Selecciona solo las columnas de tipo cadena\u001b[39;00m\n\u001b[1;32m      6\u001b[0m columnas_cadena \u001b[38;5;241m=\u001b[39m [col_name \u001b[38;5;28;01mfor\u001b[39;00m col_name, dtype \u001b[38;5;129;01min\u001b[39;00m audiencias_merch\u001b[38;5;241m.\u001b[39mdtypes \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\n\u001b[0;31mNameError\u001b[0m: name 'audiencias_merch' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'audiencias_merch' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, initcap\n",
    "# Obtiene todas las columnas\n",
    "todas_las_columnas = audiencias_merch.columns\n",
    "\n",
    "# Selecciona solo las columnas de tipo cadena\n",
    "columnas_cadena = [col_name for col_name, dtype in audiencias_merch.dtypes if dtype == \"string\"]\n",
    "\n",
    "# Lista de campos que deseas omitir\n",
    "campos_a_omitir = [\"CORREO\",'Tema',\"Tema_gral\",'T_OPERATION_TYPE','T_OPERATION_KIND','PRODUCT_CODE']\n",
    "\n",
    "# Aplica las transformaciones a las columnas de tipo cadena\n",
    "audiencias_merch = audiencias_merch.select(\n",
    "    *[initcap(col(column)).alias(column) if column in columnas_cadena and column not in campos_a_omitir else col(column) for column in todas_las_columnas]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c89739-e14e-41cc-bfb2-295c64a12185",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impresion\n",
    "# Guarda el DataFrame en formato Parquet en Azure Blob Storage\n",
    "inpath = \"abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/gold/TU_BOLETA/Audiencias_mercadeo/\"\n",
    "# 2023_fcf \n",
    "join_df_coalesced = audiencias_merch.coalesce(1)\n",
    "join_df_coalesced.write.mode(\"overwrite\").parquet(inpath)\n",
    "\n",
    "#cambio de nombre\n",
    "files=dbutils.fs.ls(inpath)\n",
    "output_file= [x for x in files if x.name.startswith(\"part-\")]\n",
    "\n",
    "# EN la misma carpeta de origen\n",
    "dbutils.fs.mv(output_file[0].path, f\"{inpath}/Ds_G_audiencias_mercadeo.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46203b56-f6e4-4514-b18d-43edd88a86c5",
     "showTitle": true,
     "title": "TABLA DE AUDIENCIAS"
    }
   },
   "outputs": [],
   "source": [
    "# audiencias = spark.sql(\n",
    "#     \"\"\"\n",
    "# SELECT\n",
    "# s.FILE_NUMBER,\n",
    "# s.T_ORGANIZ_ID as Codigo_organismo,\n",
    "# s.ORGANIZATION as Organismo,\n",
    "# s.T_ORDER_CONTACT_ID as Id_contacto,\n",
    "# c.FIRSTNAME as NOMBRE,\n",
    "# c.LASTNAME as APELLIDO,\n",
    "# c.ID_NUMBER as Cedula,\n",
    "# s.ORDER_CONTACT_NUMBER as COD_CONTACTO,\n",
    "# c.FECHA_DE_NACIMIENTO,\n",
    "# s.REFERENCE_DATE as Fecha_Compra,\n",
    "# s.PRODUCT_DATE_TIME as Fecha_hora_evento,\n",
    "# s.Fecha_evento,\n",
    "# --s.Hora_evento,\n",
    "# s.Key_evento,\n",
    "# s.ORDER_CREATION_DATE AS Hora_de_compra,\n",
    "# c.Edad,\n",
    "# c.Rango_de_edad,\n",
    "# c.EMAIL as CORREO,\n",
    "# c.NAT_NUMBER_CELLPHONE as MoVIL,\n",
    "# c.Genero,\n",
    "# c.MAIN_ADDR_LINE1 as Direccion, \n",
    "# CASE \n",
    "#     WHEN c.CIUDAD IS NULL THEN 'SIN DATO'\n",
    "#     ELSE c.CIUDAD\n",
    "#   END AS Ciudad_del_cliente,\n",
    "# --c.CIUDAD as Ciudad_del_cliente,\n",
    "# c.REGION as Region_del_cliente,\n",
    "# c.DEPARTAMENTO as Departamento_del_cliente,\n",
    "# c.CAPITAL as Ciudad_capital_del_cliente,\n",
    "# c.PAIS as Pais_del_cliente,\n",
    "# s.SALES_CHANNEL as Canal_de_venta,\n",
    "# s.SALES_CHANNEL_N,\n",
    "# s.SALES_CHANNEL_TYPE as Subcanal,\n",
    "# s.product_family as Familia_producto,\n",
    "# s.T_PRODUCT_ID as Codigo_evento,\n",
    "# s.PRODUCT as Evento,\n",
    "# s.T_PERFORMANCE_ID as Sesion,\n",
    "# --CONCAT(FORMAT_NUMBER(s.T_PRODUCT_ID, '0'), '_',FORMAT_NUMBER(s.T_PERFORMANCE_ID, '0')) AS Key_evento,\n",
    "# CONCAT(s.FILE_NUMBER,'|',CONCAT(YEAR(s.REFERENCE_DATE),'_',MONTH(s.REFERENCE_DATE))) AS Key_handling,\n",
    "# s.TEAM_1,\n",
    "# s.TEAM_2_N as TEAM_2,\n",
    "# s.LICENCE_NUMBER as Codigo_pulep,\n",
    "# s.PROMOTION as Promocion,\n",
    "# s.LOGICAL_SEAT_CATEGORY as Categoria_Logica,\n",
    "# --s.T_SITE_ID as Codigo_del_venue,\n",
    "# s.TOPIC as Tema,\n",
    "# s.SUB_TOPIC as Subtema,\n",
    "# s.SEASON as Temporada,\n",
    "# s.TOWN as Ciudad_venue,\n",
    "# c.ADDRESS_SALUTATION as Tratamiento,\n",
    "# s.SITE as Venue,\n",
    "# s.VENUE_N as Venue_N,\n",
    "# s.Tipo_venue,\n",
    "# p.PAYMENT_METHOD as Metodo_de_pago,\n",
    "# s.PRODUCT_CODE,\n",
    "# --m.PRODUCT as evento_maestra,\n",
    "# --CASE WHEN m.NUEVO_TOPIC IS NULL\n",
    "# --  THEN s.TOPIC_N\n",
    "# --  ELSE m.NUEVO_TOPIC END AS Tema_gral,\n",
    "# --CASE WHEN m.NUEVO_SUBTOPIC IS NULL\n",
    "# --  THEN s.SUB_TOPIC_N\n",
    "# --  ELSE m.NUEVO_SUBTOPIC END AS Subtema_gral,\n",
    "# s.TOPIC_PPTO as Tema_gral,\n",
    "# s.SUB_TOPIC_N as Subtema_gral,\n",
    "# s.Nuevo_aforo as Aforo,\n",
    "# s.PERFORMANCE_QUOTA as Aforo_old,\n",
    "# s.T_OPERATION_TYPE,\n",
    "# s.T_OPERATION_KIND,\n",
    "# s.AUDIENCE_CATEGORY,\n",
    "# s.T_INVOICE_CONTACT_ID as Id_promotor,\n",
    "# s.INVOICE_CONTACT_FMT_NAME as Promotor,\n",
    "# SUM(s.NET_SOLD_T_QTY)- SUM(s.NET_SOLD_C_QTY) as Tickets_comprados,\n",
    "# SUM(s.NET_SOLD_C_QTY) AS Cortesias,\n",
    "# SUM(s.NET_SOLD_TKT_AMT_ITX) AS Recaudo,\n",
    "# SUM(s.CHARGE_AMT_ITX) as Booking_fee,\n",
    "# SUM(s.CHARGE_AMT_ETX) as Booking_fee_sin_imp,\n",
    "# SUM(s.INSURED_AMT_ITX) as INSURED_AMT_ITX,\n",
    "# SUM(s.INSURED_AMT_ETX) as INSURED_AMT_ETX\n",
    "# FROM sales_vw s\n",
    "# LEFT JOIN contacts_vw c\n",
    "#   ON s.T_ORDER_CONTACT_ID = c.T_CONTACT_ID\n",
    "# LEFT JOIN dim_payments_vw p\n",
    "#   ON s.FILE_NUMBER = p.FILE_NUMBER\n",
    "# --LEFT JOIN Maestra_subtopic_2_vw m\n",
    "# --  ON s.PRODUCT = m.PRODUCT\n",
    "# WHERE YEAR(s.REFERENCE_DATE) >= 2022\n",
    "#     AND s.T_OPERATION_TYPE IN ('SALE','REFUND')\n",
    "# --\t\tAND s.T_OPERATION_KIND IN ('SINGLE_ENTRY','PRODUCT_COMPOSITION','SIMPLE_PRODUCT')--'COMPOSED_PRODUCT' \n",
    "# --\t\tAND s.PRODUCT_CODE != 'CAN_INS'\n",
    "#     --AND s.product_family IN ('Paquete','Competición','Evento','Paquetes',\n",
    "#         --'Pases con fecha y hora','Pase de visita','Pases sin fecha') \n",
    "# GROUP BY\n",
    "# s.FILE_NUMBER,s.T_ORGANIZ_ID,s.ORGANIZATION,s.T_ORDER_CONTACT_ID,c.FIRSTNAME,c.LASTNAME,c.ID_NUMBER,s.ORDER_CONTACT_NUMBER,c.FECHA_DE_NACIMIENTO,s.REFERENCE_DATE,s.PRODUCT_DATE_TIME,\n",
    "# s.Fecha_evento,s.Key_evento,--s.Hora_evento,\n",
    "# s.ORDER_CREATION_DATE,c.Edad,c.Rango_de_edad,c.EMAIL,c.NAT_NUMBER_CELLPHONE,c.Genero,c.ADDRESS_SALUTATION,c.MAIN_ADDR_LINE1,c.CIUDAD,c.REGION,c.DEPARTAMENTO,c.CAPITAL,c.PAIS,s.SALES_CHANNEL,s.SALES_CHANNEL_N,s.SALES_CHANNEL_TYPE,s.product_family,s.T_PRODUCT_ID,s.PRODUCT,s.T_PERFORMANCE_ID,s.TEAM_1,s.TEAM_2_N,s.LICENCE_NUMBER,s.PROMOTION,s.LOGICAL_SEAT_CATEGORY,s.TOPIC,s.SUB_TOPIC,s.SEASON,s.TOWN,c.ADDRESS_SALUTATION,s.SITE,s.VENUE_N,s.Tipo_venue,p.PAYMENT_METHOD,s.PRODUCT_CODE,s.TOPIC_PPTO,s.SUB_TOPIC_N,s.Nuevo_aforo,s.PERFORMANCE_QUOTA,s.T_OPERATION_TYPE,s.T_OPERATION_KIND,s.AUDIENCE_CATEGORY,s.T_INVOICE_CONTACT_ID,s.INVOICE_CONTACT_FMT_NAME\n",
    "# \"\"\"\n",
    "# )\n",
    "\n",
    "# audiencias.count()\n",
    "# #display(audiencias.limit(10))\n",
    "# audiencias.createOrReplaceTempView(\"audiencias_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f02b47-d7cd-409b-abd8-c707204528f4",
     "showTitle": true,
     "title": "TABLA AUDIENCIAS"
    }
   },
   "outputs": [],
   "source": [
    "audiencias = spark.sql(\n",
    "    \"\"\"\n",
    "\n",
    "WITH Ranked_Values AS (\n",
    "  -- Ajuste asistencias Daniel Hernandez 7-may-2024\n",
    "  SELECT\n",
    "    s.FILE_NUMBER,\n",
    "    s.INSURED_AMT_ETX,\n",
    "    ROW_NUMBER() OVER (PARTITION BY s.FILE_NUMBER ORDER BY s.INSURED_AMT_ETX DESC) AS rn\n",
    "  FROM sales_vw s\n",
    "  WHERE s.INSURED_AMT_ETX != 0 AND YEAR(s.REFERENCE_DATE) >= 2022 AND s.T_OPERATION_TYPE IN ('SALE','REFUND')\n",
    ")\n",
    "SELECT\n",
    "s.FILE_NUMBER,\n",
    "s.T_ORGANIZ_ID as Codigo_organismo,\n",
    "s.ORGANIZATION as Organismo,\n",
    "s.T_ORDER_CONTACT_ID as Id_contacto,\n",
    "c.FIRSTNAME as NOMBRE,\n",
    "c.LASTNAME as APELLIDO,\n",
    "c.ID_NUMBER as Cedula,\n",
    "s.ORDER_CONTACT_NUMBER as COD_CONTACTO,\n",
    "c.FECHA_DE_NACIMIENTO,\n",
    "s.REFERENCE_DATE as Fecha_Compra,\n",
    "s.PRODUCT_DATE_TIME as Fecha_hora_evento,\n",
    "s.Fecha_evento,\n",
    "--s.Hora_evento,\n",
    "s.Key_evento,\n",
    "s.Key_aforo,\n",
    "s.ORDER_CREATION_DATE AS Hora_de_compra,\n",
    "c.Edad,\n",
    "c.Rango_de_edad,\n",
    "c.EMAIL as CORREO,\n",
    "c.NAT_NUMBER_CELLPHONE as MoVIL,\n",
    "c.Genero,\n",
    "c.MAIN_ADDR_LINE1 as Direccion, \n",
    "CASE \n",
    "    WHEN c.CIUDAD IS NULL THEN 'SIN DATO'\n",
    "    ELSE c.CIUDAD\n",
    "  END AS Ciudad_del_cliente,\n",
    "--c.CIUDAD as Ciudad_del_cliente,\n",
    "c.REGION as Region_del_cliente,\n",
    "c.DEPARTAMENTO as Departamento_del_cliente,\n",
    "c.CAPITAL as Ciudad_capital_del_cliente,\n",
    "c.PAIS as Pais_del_cliente,\n",
    "s.SALES_CHANNEL as Canal_de_venta,\n",
    "s.SALES_CHANNEL_N,\n",
    "s.SALES_CHANNEL_TYPE as Subcanal,\n",
    "s.product_family as Familia_producto,\n",
    "s.T_PRODUCT_ID as Codigo_evento,\n",
    "s.PRODUCT_N as Evento,\n",
    "s.T_PERFORMANCE_ID as Sesion,\n",
    "--CONCAT(FORMAT_NUMBER(s.T_PRODUCT_ID, '0'), '_',FORMAT_NUMBER(s.T_PERFORMANCE_ID, '0')) AS Key_evento,\n",
    "s.Key_handling,\n",
    "s.TEAM_1,\n",
    "s.TEAM_2_N as TEAM_2,\n",
    "s.LICENCE_NUMBER as Codigo_pulep,\n",
    "s.PROMOTION as Promocion,\n",
    "s.LOGICAL_SEAT_CATEGORY as Categoria_Logica,\n",
    "--s.T_SITE_ID as Codigo_del_venue,\n",
    "s.TOPIC as Tema,\n",
    "s.SUB_TOPIC as Subtema,\n",
    "s.SEASON as Temporada,\n",
    "s.TOWN_N as Ciudad_venue,\n",
    "c.ADDRESS_SALUTATION as Tratamiento,\n",
    "s.SITE as Venue,\n",
    "s.VENUE_N as Venue_N,\n",
    "s.Tipo_venue,\n",
    "p.PAYMENT_METHOD as Metodo_de_pago,\n",
    "s.PRODUCT_CODE,\n",
    "--m.PRODUCT as evento_maestra,\n",
    "--CASE WHEN m.NUEVO_TOPIC IS NULL\n",
    "--  THEN s.TOPIC_N\n",
    "--  ELSE m.NUEVO_TOPIC END AS Tema_gral,\n",
    "--CASE WHEN m.NUEVO_SUBTOPIC IS NULL\n",
    "--  THEN s.SUB_TOPIC_N\n",
    "--  ELSE m.NUEVO_SUBTOPIC END AS Subtema_gral,\n",
    "s.TOPIC_PPTO as Tema_gral,\n",
    "s.SUB_TOPIC_N as Subtema_gral,\n",
    "s.Nuevo_aforo as Aforo,\n",
    "s.PERFORMANCE_QUOTA as Aforo_old,\n",
    "s.T_OPERATION_TYPE,\n",
    "s.T_OPERATION_KIND,\n",
    "s.AUDIENCE_CATEGORY,\n",
    "s.T_INVOICE_CONTACT_ID as Id_promotor,\n",
    "s.INVOICE_CONTACT_FMT_NAME as Promotor,\n",
    "SUM(s.NET_SOLD_T_QTY)- SUM(s.NET_SOLD_C_QTY) as Tickets_comprados,\n",
    "SUM(s.NET_SOLD_C_QTY) AS Cortesias,\n",
    "SUM(s.NET_SOLD_TKT_AMT_ITX) AS Recaudo,\n",
    "SUM(s.CHARGE_AMT_ITX) as Booking_fee,\n",
    "SUM(s.CHARGE_AMT_ETX) as Booking_fee_sin_imp,\n",
    "SUM(s.INSURED_AMT_ITX) as INSURED_AMT_ITX,\n",
    "SUM(s.Hfee) as Hfee,\n",
    "rv.INSURED_AMT_ETX AS INSURED_AMT_ETX,\n",
    "s.t_file_state\n",
    "FROM sales_vw s\n",
    "LEFT JOIN contacts_vw c\n",
    "  ON s.T_ORDER_CONTACT_ID = c.T_CONTACT_ID\n",
    "LEFT JOIN dim_payments_vw p\n",
    "  ON s.FILE_NUMBER = p.FILE_NUMBER\n",
    "LEFT JOIN Ranked_Values rv\n",
    "  ON s.FILE_NUMBER = rv.FILE_NUMBER AND rv.rn = 1\n",
    "--LEFT JOIN Maestra_subtopic_2_vw m\n",
    "--  ON s.PRODUCT = m.PRODUCT\n",
    "WHERE YEAR(s.REFERENCE_DATE) >= 2022\n",
    "    AND s.T_OPERATION_TYPE IN ('SALE','REFUND')\n",
    "--\t\tAND s.T_OPERATION_KIND IN ('SINGLE_ENTRY','PRODUCT_COMPOSITION','SIMPLE_PRODUCT')--'COMPOSED_PRODUCT' \n",
    "--\t\tAND s.PRODUCT_CODE != 'CAN_INS'\n",
    "    --AND s.product_family IN ('Paquete','Competición','Evento','Paquetes',\n",
    "        --'Pases con fecha y hora','Pase de visita','Pases sin fecha') \n",
    "GROUP BY\n",
    "s.FILE_NUMBER,s.T_ORGANIZ_ID,s.ORGANIZATION,s.T_ORDER_CONTACT_ID,c.FIRSTNAME,c.LASTNAME,c.ID_NUMBER,s.ORDER_CONTACT_NUMBER,c.FECHA_DE_NACIMIENTO,s.REFERENCE_DATE,s.PRODUCT_DATE_TIME,\n",
    "s.Fecha_evento,s.Key_evento,s.Key_aforo,--s.Hora_evento,\n",
    "s.ORDER_CREATION_DATE,c.Edad,c.Rango_de_edad,c.EMAIL,c.NAT_NUMBER_CELLPHONE,c.Genero,c.ADDRESS_SALUTATION,c.MAIN_ADDR_LINE1,c.CIUDAD,c.REGION,c.DEPARTAMENTO,c.CAPITAL,c.PAIS,s.SALES_CHANNEL,s.SALES_CHANNEL_N,s.SALES_CHANNEL_TYPE,s.product_family,s.T_PRODUCT_ID,s.PRODUCT_N,s.T_PERFORMANCE_ID,s.Key_handling,s.TEAM_1,s.TEAM_2_N,s.LICENCE_NUMBER,s.PROMOTION,s.LOGICAL_SEAT_CATEGORY,s.TOPIC,s.SUB_TOPIC,s.SEASON,s.TOWN_N,c.ADDRESS_SALUTATION,s.SITE,s.VENUE_N,s.Tipo_venue,p.PAYMENT_METHOD,s.PRODUCT_CODE,s.TOPIC_PPTO,s.SUB_TOPIC_N,s.Nuevo_aforo,s.PERFORMANCE_QUOTA,s.T_OPERATION_TYPE,s.T_OPERATION_KIND,s.AUDIENCE_CATEGORY,s.T_INVOICE_CONTACT_ID,s.INVOICE_CONTACT_FMT_NAME, rv.INSURED_AMT_ETX,s.t_file_state\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# audiencias.count()\n",
    "#display(audiencias.limit(10))\n",
    "audiencias.createOrReplaceTempView(\"audiencias_vw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab010fd0-c5b3-420a-9e06-9ec1f93ed0fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## AJUSTE DEL CAMPO (INSURED_AMT_ETX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "127a44ec-213c-4e22-ba43-1e232bcb4777",
     "showTitle": true,
     "title": "Eliminar registros extras de INSURED_AMT_ETX"
    }
   },
   "outputs": [],
   "source": [
    "audiencias = audiencias.withColumn(\n",
    "    \"INSURED_AMT_ETX\",\n",
    "    when(audiencias.INSURED_AMT_ITX == 0, 0)\n",
    "    .otherwise(audiencias.INSURED_AMT_ETX)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab62e43a-d66e-4cb9-929d-70a919f93067",
     "showTitle": true,
     "title": "Imputacion del seguro"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, max as max_\n",
    "\n",
    "# Definir una ventana que particione por FILE_NUMBER\n",
    "window_spec = Window.partitionBy(\"FILE_NUMBER\")\n",
    "\n",
    "# Agregar una columna que contenga el valor de 'recaudo' donde las condiciones específicas se cumplen\n",
    "audiencias = audiencias.withColumn(\n",
    "    \"recaudo_CAN_INS_SALE\",\n",
    "    max_(when((col(\"PRODUCT_CODE\") == \"CAN_INS\") & (col(\"T_OPERATION_TYPE\") == \"SALE\"), col(\"recaudo\"))).over(window_spec)\n",
    ")\n",
    "\n",
    "audiencias = audiencias.withColumn(\n",
    "    \"recaudo_CAN_INS_REFUND\",\n",
    "    max_(when((col(\"PRODUCT_CODE\") == \"CAN_INS\") & (col(\"T_OPERATION_TYPE\") == \"REFUND\"), col(\"recaudo\"))).over(window_spec)\n",
    ")\n",
    "\n",
    "audiencias = audiencias.withColumn(\n",
    "    \"INSURED_AMT_ETX\",\n",
    "    when(\n",
    "        (col(\"PRODUCT_CODE\") != \"CAN_INS\") & \n",
    "        (col(\"PRODUCT_CODE\").isNotNull()) & \n",
    "        (col(\"T_OPERATION_TYPE\") == \"SALE\") &\n",
    "        (col(\"recaudo_CAN_INS_SALE\").isNotNull()),\n",
    "        col(\"recaudo_CAN_INS_SALE\")/1.19\n",
    "    ).when(\n",
    "        (col(\"PRODUCT_CODE\") != \"CAN_INS\") & \n",
    "        (col(\"PRODUCT_CODE\").isNotNull()) & \n",
    "        (col(\"T_OPERATION_TYPE\") == \"REFUND\") &\n",
    "        (col(\"recaudo_CAN_INS_REFUND\").isNotNull()),\n",
    "        col(\"recaudo_CAN_INS_REFUND\")/1.19        \n",
    "    ).otherwise(col(\"INSURED_AMT_ETX\"))\n",
    ")\n",
    "\n",
    "\n",
    "audiencias = audiencias.withColumn(\n",
    "    \"INSURED_AMT_ETX\",\n",
    "    when(\n",
    "        (col(\"T_OPERATION_KIND\") == \"PRODUCT_COMPOSITION\"),0       \n",
    "    ).otherwise(col(\"INSURED_AMT_ETX\"))\n",
    ")\n",
    "\n",
    "# Eliminar la columna temporal 'recaudo_CAN_INS_SALE'\n",
    "audiencias = audiencias.drop(\"recaudo_CAN_INS_SALE\",\"recaudo_CAN_INS_REFUND\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8d4fabc-36ed-47a8-8fa5-e63428bac06d",
     "showTitle": true,
     "title": "Correción el INSURED_AMT_ETX cuando son refunds V.2 (Ingresar el valor de la devolucion)"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, max as max_\n",
    "\n",
    "## REFUNDS DEL SEGURO NO VALIDOS \n",
    "# Marcar las filas que cumplen las condiciones\n",
    "audiencias = audiencias.withColumn(\n",
    "    \"valid_refund\",\n",
    "    when((col(\"PRODUCT_CODE\") == \"CAN_INS\") & (col(\"T_OPERATION_TYPE\") == \"REFUND\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Usar una ventana para determinar si existe al menos una fila válida en cada grupo FILE_NUMBER\n",
    "window_spec = Window.partitionBy(\"FILE_NUMBER\")\n",
    "audiencias = audiencias.withColumn(\n",
    "    \"has_valid_refund\",\n",
    "    max_(\"valid_refund\").over(window_spec)\n",
    ")\n",
    "\n",
    "## SE SIGUE CON EL CODIGO ORIGINAL\n",
    "# Filtrar por las condiciones específicas\n",
    "filtered_df = audiencias.filter(\n",
    "    audiencias.T_OPERATION_KIND.isin(\"SINGLE_PRODUCT\", \"SINGLE_ENTRY\", \"COMPOSED_PRODUCT\")\n",
    ")\n",
    "\n",
    "# Usar una ventana para determinar el valor máximo a aplicar en insured_amt_etx\n",
    "window_spec = Window.partitionBy(\"FILE_NUMBER\", \"Codigo_evento\")\n",
    "\n",
    "\n",
    "# Preparar un DataFrame con los valores máximos por grupo\n",
    "max_values_df = (\n",
    "    filtered_df.withColumn(\n",
    "        \"max_insured_amt_etx\",\n",
    "        max_(\"insured_amt_etx\").over(window_spec)\n",
    "    ).withColumn(\n",
    "        \"max_insured_amt_etx\",\n",
    "        when(col(\"has_valid_refund\") == 0, 0).otherwise(col(\"max_insured_amt_etx\"))\n",
    "    ).select(col(\"max_insured_amt_etx\"), col(\"file_number\").alias(\"FILE_NUMBER_B\"), col(\"Codigo_evento\").alias(\"Codigo_evento_B\"), col(\"max_insured_amt_etx\").alias(\"MAX_INSURED_AMT_ETX_B\"),col(\"T_OPERATION_TYPE\").alias(\"T_OPERATION_TYPE_B\")).distinct() \n",
    "    # Asegurarse de que no haya duplicados en este DataFrame\n",
    ")\n",
    "\n",
    "# Hacer join con el DataFrame original para actualizar los valores\n",
    "audiencias = audiencias.join(\n",
    "    max_values_df,\n",
    "    (audiencias[\"file_number\"] == max_values_df[\"file_number_B\"]) &\n",
    "    (audiencias[\"Codigo_evento\"] == max_values_df[\"Codigo_evento_B\"]) &\n",
    "    (audiencias[\"T_OPERATION_TYPE\"] == max_values_df[\"T_OPERATION_TYPE_B\"]) &\n",
    "    (max_values_df[\"T_OPERATION_TYPE_B\"] == \"REFUND\"),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Actualizar los valores de insured_amt_etx con los nuevos valores máximos\n",
    "audiencias = audiencias.withColumn(\n",
    "    \"insured_amt_etx\",\n",
    "    when(\n",
    "        col(\"max_insured_amt_etx\").isNotNull(),\n",
    "        col(\"max_insured_amt_etx\")\n",
    "    ).otherwise(col(\"insured_amt_etx\"))\n",
    ")\n",
    "\n",
    "# Si aún observas duplicados, aplicar distinct aquí puede ayudar, pero con precaución\n",
    "# audiencias = audiencias.distinct()\n",
    "\n",
    "audiencias = audiencias.withColumn(\n",
    "    \"insured_amt_etx\",\n",
    "    when(\n",
    "        (col(\"t_operation_type\") == \"REFUND\") &\n",
    "        (col(\"max_insured_amt_etx\").isNotNull()),                                                 \n",
    "        col(\"insured_amt_etx\") * -1\n",
    "    ).otherwise(col(\"insured_amt_etx\"))\n",
    ")\n",
    "\n",
    "col_eliminar = ['FILE_NUMBER_B','Codigo_evento_B','MAX_INSURED_AMT_ETX_B','T_OPERATION_TYPE_B',\"valid_refund\", \"has_valid_refund\",\"max_insured_amt_etx\"]\n",
    "\n",
    "audiencias = audiencias.drop(*col_eliminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24f0ef26-8e01-4190-8017-e055586ed285",
     "showTitle": true,
     "title": "Ponderancion del INSURED_AMT_ETX"
    }
   },
   "outputs": [],
   "source": [
    "# # Modificación inicial para establecer 'recaudo_condicional'\n",
    "audiencias = audiencias.withColumn(\n",
    "    'recaudo_condicional',\n",
    "    when(\n",
    "        (col('product_code').isNotNull()) & (col('product_code') != 'CAN_INS') & (col('T_OPERATION_KIND') != 'PRODUCT_COMPOSITION'),\n",
    "        col('recaudo')\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Definir la ventana para la creacion de columna recaudo_2\n",
    "window_spec_2 = Window.partitionBy(\"FILE_NUMBER\",'T_OPERATION_TYPE') #Creacion ventana adicional por caso file = 11972945\n",
    "\n",
    "# Calcular la suma acumulada solo de valores positivos\n",
    "audiencias = audiencias.withColumn(\n",
    "    'recaudo_suma_acumulada_positivos_2',\n",
    "    sum(col('recaudo_condicional')).over(window_spec_2)\n",
    ")\n",
    "\n",
    "# Definir una ventana que considere todas las filas anteriores desde el inicio del DataFrame\n",
    "windowSpec = Window.partitionBy('File_Number')\n",
    "\n",
    "# Calcular la suma acumulada solo de valores positivos\n",
    "audiencias = audiencias.withColumn(\n",
    "    'recaudo_suma_acumulada_positivos',\n",
    "    sum(when(col('recaudo_condicional') > 0, col('recaudo_condicional')).otherwise(0)).over(windowSpec)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Calcular la suma acumulada solo de valores negativos\n",
    "audiencias = audiencias.withColumn(\n",
    "    'recaudo_suma_acumulada_negativos',\n",
    "    sum(when(col('recaudo_condicional') < 0, col('recaudo_condicional')).otherwise(0)).over(windowSpec)\n",
    ")\n",
    "\n",
    "audiencias = audiencias.withColumn(\n",
    "    'recaudo_suma_neg_pos',\n",
    "    col('recaudo_suma_acumulada_positivos') + col('recaudo_suma_acumulada_negativos')\n",
    ")\n",
    "\n",
    "audiencias = audiencias.withColumn(\n",
    "    \"recaudo_2\",\n",
    "    sum(col(\"recaudo_suma_acumulada_positivos\")).over(window_spec)\n",
    ")\n",
    "\n",
    "# Usar los valores acumulados para ajustar 'INSURED_AMT_ETX'\n",
    "audiencias = audiencias.withColumn(\n",
    "    'INSURED_AMT_ETX',\n",
    "    when(\n",
    "        col('recaudo_suma_neg_pos') == 0,\n",
    "        when(\n",
    "            col('recaudo_condicional') > 0,\n",
    "            (col('INSURED_AMT_ETX') / col('recaudo_suma_acumulada_positivos')) * col('recaudo')\n",
    "        ).when(\n",
    "            col('recaudo_condicional') < 0,\n",
    "            (col('INSURED_AMT_ETX') / col('recaudo_suma_acumulada_negativos')) * col('recaudo')\n",
    "        ).otherwise(col('INSURED_AMT_ETX'))\n",
    "    ).otherwise(\n",
    "            (col('INSURED_AMT_ETX') / col('recaudo_suma_acumulada_positivos_2')) * col('recaudo')\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "col_eliminar = ['recaudo_condicional','recaudo_suma_acumulada','recaudo_suma_acumulada_positivos','recaudo_suma_acumulada_negativos','recaudo_2','recaudo_suma_neg_pos','recaudo_suma_acumulada_positivos_2']\n",
    "\n",
    "audiencias = audiencias.drop(*col_eliminar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d2d4a31-6ba5-4efb-acb4-65453db3fc96",
     "showTitle": true,
     "title": "Arreglo del campo INSURED_AMT_ETX para el caso #2 de los refunds (No esta la fila para agregarle el refund)"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, max, when\n",
    "\n",
    "### PASO 1: Duplicar filas con determinadas condiciones\n",
    "\n",
    "# Definir una ventana por FILE_NUMBER\n",
    "window_spec = Window.partitionBy(\"FILE_NUMBER\")\n",
    "\n",
    "# Crear columnas que me van a ayudar identificar las filas a duplicar\n",
    "df = audiencias.withColumn(\"has_refund\",\n",
    "                   max(when((col(\"PRODUCT_CODE\") == \"CAN_INS\") & (col(\"T_OPERATION_TYPE\") == \"REFUND\"), 1).otherwise(0)).over(window_spec))\n",
    "\n",
    "df = df.withColumn(\"has_valid_event\",\n",
    "                   max(when((col(\"Fecha_hora_evento\").isNotNull()) & (col(\"T_OPERATION_TYPE\") == \"REFUND\"), 1).otherwise(0)).over(window_spec))\n",
    "\n",
    "# Combinar las condiciones en una nueva columna 'refund_count'\n",
    "df = df.withColumn(\"refund_count\", when((col(\"has_refund\") == 1) & (col(\"has_valid_event\") == 0), 1).otherwise(0))\n",
    "\n",
    "# Filtrar y duplicar filas bajo condiciones específicas\n",
    "df_to_duplicate = df.filter(\n",
    "    (col(\"refund_count\") == 1) &\n",
    "    (col(\"T_OPERATION_KIND\").isin([\"SINGLE_ENTRY\", \"SIMPLE_PRODUCT\", \"COMPOSED_PRODUCT\"])) &\n",
    "    (col(\"Codigo_evento\").isNotNull()) &\n",
    "    (col('PRODUCT_CODE')!='CAN_INS')\n",
    ")\n",
    "\n",
    "# Añadir una columna nueva que identifique filas duplicadas para ambos DataFrames\n",
    "df = df.withColumn(\"duplicated\", lit(0))\n",
    "duplicated_df = df_to_duplicate.withColumn(\"duplicated\", lit(1))\n",
    "\n",
    "# Unir las filas duplicadas al DataFrame original\n",
    "final_df = df.union(duplicated_df).drop(\"has_refund\", \"has_valid_event\", \"refund_count\")\n",
    "\n",
    "\n",
    "### PASO 2: Pasar a 0 los valores de filas duplicadas\n",
    "columns_to_zero = [\"INSURED_AMT_ITX\", \"Booking_fee_sin_imp\", \"Booking_fee\", \"Recaudo\", \"Cortesias\", \"Tickets_comprados\"]\n",
    "for column in columns_to_zero:\n",
    "    final_df = final_df.withColumn(column, when(col(\"duplicated\") == 1, 0).otherwise(col(column)))\n",
    "\n",
    "\n",
    "\n",
    "### PASO 3: Multiplicar *-1 INSURED_AMT_ETX\n",
    "final_df = final_df.withColumn(\"INSURED_AMT_ETX\", when(col(\"duplicated\") == 1, col(\"INSURED_AMT_ETX\") * -1).otherwise(col(\"INSURED_AMT_ETX\")))\n",
    "\n",
    "\n",
    "\n",
    "### Paso 4: Cambiar la fecha de compra (JULIAN FILTRA EL CONSOLIDADO DE MES POR ESTE CAMPO / NO SE EL RESTO DE CAMPOS SI TAMBIEN SE DEBAN DE CAMBIAR EN ALGO)\n",
    "## COL 1\n",
    "# Definir una ventana por FILE_NUMBER\n",
    "window_spec = Window.partitionBy(\"FILE_NUMBER\")\n",
    "\n",
    "# Encontrar el valor de 'Fecha_hora_compra' para las filas que son 'CAN_INS' y 'Refund'\n",
    "final_df = final_df.withColumn(\"ref_fecha_hora_compra\",\n",
    "                               max(when((col(\"PRODUCT_CODE\") == \"CAN_INS\") & (col(\"T_OPERATION_TYPE\") == \"REFUND\"),\n",
    "                                          col(\"Fecha_Compra\")).otherwise(None)).over(window_spec))\n",
    "\n",
    "# Actualizar la columna 'Fecha_hora_compra' en las filas duplicadas\n",
    "final_df = final_df.withColumn(\"Fecha_Compra\",\n",
    "                               when(col(\"duplicated\") == 1, col(\"ref_fecha_hora_compra\")).otherwise(col(\"Fecha_Compra\")))\n",
    "\n",
    "## COL 2\n",
    "# Encontrar el valor de 'Fecha_hora_compra' para las filas que son 'CAN_INS' y 'Refund'\n",
    "final_df = final_df.withColumn(\"ref_fecha_hora_compra_2\",\n",
    "                               max(when((col(\"PRODUCT_CODE\") == \"CAN_INS\") & (col(\"T_OPERATION_TYPE\") == \"REFUND\"),\n",
    "                                          col(\"Hora_de_compra\")).otherwise(None)).over(window_spec))\n",
    "\n",
    "# Actualizar la columna 'Fecha_hora_compra' en las filas duplicadas\n",
    "final_df = final_df.withColumn(\"Hora_de_compra\",\n",
    "                               when(col(\"duplicated\") == 1, col(\"ref_fecha_hora_compra_2\")).otherwise(col(\"Hora_de_compra\")))\n",
    "\n",
    "\n",
    "## COL 3\n",
    "# Encontrar el valor de 'Fecha_hora_compra' para las filas que son 'CAN_INS' y 'Refund'\n",
    "final_df = final_df.withColumn(\"ref_fecha_hora_compra_3\",\n",
    "                               max(when((col(\"PRODUCT_CODE\") == \"CAN_INS\") & (col(\"T_OPERATION_TYPE\") == \"REFUND\"),\n",
    "                                          col(\"T_OPERATION_TYPE\")).otherwise(None)).over(window_spec))\n",
    "\n",
    "# Actualizar la columna 'Fecha_hora_compra' en las filas duplicadas\n",
    "final_df = final_df.withColumn(\"T_OPERATION_TYPE\",\n",
    "                               when(col(\"duplicated\") == 1, col(\"ref_fecha_hora_compra_3\")).otherwise(col(\"T_OPERATION_TYPE\")))\n",
    "\n",
    "\n",
    "# Columnas a eliminar\n",
    "col_eliminar = ['ref_fecha_hora_compra','ref_fecha_hora_compra_2','ref_fecha_hora_compra_3']\n",
    "\n",
    "final_df = final_df.drop(*col_eliminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e722f2ef-bf4f-4dd5-8d86-6becab03cfda",
     "showTitle": true,
     "title": "PRUEBAS"
    }
   },
   "outputs": [],
   "source": [
    "# display(final_df.filter(final_df[\"FILE_NUMBER\"]==\"15534293\")) #REFUND - CAMBIO EVENTO, NO SE REGRESA SEGURO                                         !!!!!!!\n",
    "# display(final_df.filter(final_df[\"FILE_NUMBER\"]==\"15625275\")) # COMPRO 2 EVENTOS QUE SE LLAMAN IGUAL PERO EN SESIONES DIFERENTES                ******\n",
    "# display(final_df.filter(final_df['FILE_NUMBER']=='11409182')) #REFUND DE TODAS LAS BOLETAS, NO SE REGERSA EL SEGURO                 \n",
    "# display(final_df.filter(final_df['FILE_NUMBER']=='16669980')) #COMPRA VARIOS EVENTOS                                                ******\n",
    "# display(final_df.filter(final_df['FILE_NUMBER']=='14854919')) #REFUND DE LAS BOLETAS Y EL SEGURO                                \n",
    "# display(final_df.filter(final_df['FILE_NUMBER']=='15440159'))# REFUND DE ALGUNAS BOLETAS, NO SE REGRESA EL SEGURO               *****\n",
    "\n",
    "# display(final_df.filter(final_df[\"FILE_NUMBER\"]==\"15506697\")) #VER SUMA POSITIVA  # EL TEMA DE PRODUCT COMPOSITION, NO SE SI ESO CAUSO Q SE HAYA TIRADO TODOOOOO, TOCA VER LOS EJEMPLOS A VER Q\n",
    "# display(final_df.filter(final_df[\"FILE_NUMBER\"]==\"15514192\")) #CASO MILLONARIOS. NO TRAE NINGUNA INFO EN EL CAMPO INSURED :,(\n",
    "\n",
    "# display(final_df.filter(final_df['FILE_NUMBER']=='15531324')) #CASO PONDERACION DEL MISMO EVENTO PERO LOCALIDAD DISTINTA                    ******\n",
    "# display(final_df.filter(final_df['FILE_NUMBER']=='15464650')) #REFUND DE ALGUNAS BOLETAS (CAMBIO DE FECHA), NO SE REGRESA PLATA DEL SEGURO              !!!!!!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ###NO ANOTE TODOS LOS CASOS, SON MUCHOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6638350-c495-4734-964d-c8c004045309",
     "showTitle": true,
     "title": "Comparacion por expediente por mes (INSURED_AMT_ETX vs CAN_INS)"
    }
   },
   "outputs": [],
   "source": [
    "# ## TABLA SALES\n",
    "# #inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/temp/Daniel/audiencias/Ds_G_audiencias.parquet'\n",
    "# #df1 = spark.read.parquet(inpath)\n",
    "# from pyspark.sql.functions import col, sum, when\n",
    "\n",
    "# df1 = final_df.withColumn(\"Fecha_Compra\", to_timestamp(\"Fecha_Compra\"))\n",
    "\n",
    "# # Filtrar para obtener solo las filas de enero\n",
    "# df1 = df1.filter((month(col(\"Fecha_Compra\")) == 3)&(year(col(\"Fecha_Compra\"))==2024))\n",
    "# # Realizar ambas agregaciones en una sola operación groupBy\n",
    "# df1 = df1.groupBy(\"FILE_NUMBER\").agg(\n",
    "#     sum(\"INSURED_AMT_ETX\").alias(\"suma_valor\"),\n",
    "#     (sum(when(col(\"product_code\") == \"CAN_INS\", col(\"recaudo\")))/1.19).alias(\"suma_recaudo_CAN_INS\")\n",
    "# )\n",
    "\n",
    "# df1 = df1.withColumn(\"dif\",col(\"suma_valor\")-col(\"suma_recaudo_CAN_INS\"))\n",
    "\n",
    "# # Mostrar el resultado\n",
    "# display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6d62e4-f5b1-43e2-95a4-e57df5234a17",
     "showTitle": true,
     "title": "Comparacion por suma del mes (INSURED_AMT_ETX vs CAN_INS)"
    }
   },
   "outputs": [],
   "source": [
    "# total_sum = df1.agg(sum(\"suma_valor\").alias(\"suma_valor\"))\n",
    "# # Mostrar el resultado\n",
    "# total_sum.show()\n",
    "\n",
    "\n",
    "# total_sum2 = df1.agg(sum(\"suma_recaudo_CAN_INS\").alias(\"suma_recaudo_CAN_INS\"))\n",
    "# # Mostrar el resultado\n",
    "# total_sum2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9586ad7-5b1b-4db8-9610-7f2c583d4f0c",
     "showTitle": true,
     "title": "CASOS QUE NO SE PUDIERON SOLUCIONAR (EJEMPLO)"
    }
   },
   "outputs": [],
   "source": [
    "# # Ejemplos de expedientes que no se como arreglar, casos minimos pero que por como se comportan (la fecha de compra del seguro y los eventos son distintas) sale la plata en uno u otro mes\n",
    "# # La fecha esta cruzada (hago devolucion de boletas y seguro. Si esta el espacio, yo no le cambio la fecha (Este caso en particular creo q Paila)) o toca evaluar con julian y kevin\n",
    "\n",
    "# display(audiencias.filter(audiencias['FILE_NUMBER']=='16209004')) \n",
    "# display(final_df.filter(final_df['FILE_NUMBER']=='11972945'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf920c7-e4dc-4bec-a7d9-650e747a7abe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## FIN AJUSTE CAMPO INSURED_AMT_ETX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cbea509-e0e4-4683-8f7e-8701755399da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tmp = audiencias.select('Sesion','TEAM_2').\\\n",
    "#     filter(col('Sesion') == '10229481277847').distinct()\n",
    "\n",
    "# #display(tmp.limit(100))\n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e52915f-d1b2-4477-b419-f0fabb872402",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# tmp = spark.sql(\n",
    "#     \"\"\"\n",
    "# SELECT DISTINCT\n",
    "#   Tema,\n",
    "#   Tema_gral\n",
    "# FROM audiencias_vw\n",
    "# WHERE YEAR(Fecha_evento) IN (2020)\n",
    "# \"\"\")\n",
    "\n",
    "# #print(tmp.count()) \n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0d8ce4-7416-408e-af6a-97cedbeabd55",
     "showTitle": true,
     "title": "Agregar fecha de actualizacion"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "audiencias = final_df.withColumn(\n",
    "    \"Fecha Actualizacion\",\n",
    "    expr(\"current_timestamp() - interval 5 hours\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e9be5f-b2c1-42b8-a87d-998bca210dbd",
     "showTitle": true,
     "title": "Ajuste de medios de pago"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "archivo_csv = \"abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/temp/metodo/metodos_pago_2.csv\"\n",
    "df = spark.read.csv(archivo_csv, header=True, inferSchema=True, sep=';',encoding=\"ISO-8859-1\")\n",
    "\n",
    "# # Renombra la columna en `df` para evitar conflictos de nombres\n",
    "df = df.withColumnRenamed(\"Metodo_de_pago\", \"Metodo_de_pago_2\")\n",
    "\n",
    "# Realiza un join basado en la columna \"Metodo_de_pago\" utilizando alias\n",
    "audiencias = audiencias.alias(\"a\").join(\n",
    "    df.alias(\"b\"),\n",
    "    col(\"a.Metodo_de_pago\") == col(\"b.Metodo_de_pago_2\"),\n",
    "    \"left_outer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2da2eb8f-dbc2-4b55-95f1-aa1306e685a4",
     "showTitle": true,
     "title": "Ajustando mayusculas"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, initcap\n",
    "# Obtiene todas las columnas\n",
    "todas_las_columnas = audiencias.columns\n",
    "\n",
    "# Selecciona solo las columnas de tipo cadena\n",
    "columnas_cadena = [col_name for col_name, dtype in audiencias.dtypes if dtype == \"string\"]\n",
    "\n",
    "# Lista de campos que deseas omitir\n",
    "campos_a_omitir = [\"CORREO\",'Tema',\"Tema_gral\",'T_OPERATION_TYPE','T_OPERATION_KIND','PRODUCT_CODE']\n",
    "\n",
    "# # Aplica las transformaciones a las columnas de tipo cadena\n",
    "# audiencias_transformado = audiencias.select(\n",
    "#     *[initcap(col(column)).alias(column) if column in columnas_cadena and column != \"CORREO\" else col(column) for column in todas_las_columnas]\n",
    "# )\n",
    "\n",
    "# Aplica las transformaciones a las columnas de tipo cadena, omitiendo los campos especificados\n",
    "audiencias_transformado = audiencias.select(\n",
    "    *[initcap(col(column)).alias(column) if column in columnas_cadena and column not in campos_a_omitir else col(column) for column in todas_las_columnas]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94be5d19-4cff-412d-8f42-bd2015a8bc53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## MAESTRA LOCALIDADES AGRUPADAS MOVISTAR\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/Maestras/Localidades_Movistar/localidades_agrupadas_movistar.csv'\n",
    "localidad_movistar = spark.read.csv(inpath, sep = '|', header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "audiencias_transformado = audiencias_transformado.join(localidad_movistar,\\\n",
    "    audiencias_transformado[\"Categoria_Logica\"] == localidad_movistar[\"LOGICAL_SEAT_CATEGORY\"],\\\n",
    "    \"left_outer\").\\\n",
    "    drop('LOGICAL_SEAT_CATEGORY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14641de3-3f83-4c61-80c5-06fa67cadb0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impresion\n",
    "# Guarda el DataFrame en formato Parquet en Azure Blob Storage\n",
    "inpath = \"abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/gold/TU_BOLETA/Audiencias/\"\n",
    "# 2023_fcf\n",
    "join_df_coalesced = audiencias_transformado.coalesce(1)\n",
    "join_df_coalesced.write.mode(\"overwrite\").parquet(inpath)\n",
    "\n",
    "\n",
    "#cambio de nombre\n",
    "files=dbutils.fs.ls(inpath)\n",
    "output_file= [x for x in files if x.name.startswith(\"part-\")]\n",
    "\n",
    "# EN la misma carpeta de origen\n",
    "dbutils.fs.mv(output_file[0].path, f\"{inpath}/Ds_G_audiencias.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ac5b39-e33f-4b7e-bb08-8617238d6d44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#filter((Audiencias.Codigo_organismo == '101385877267') & (year(Audiencias.Fecha_Compra) == 2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45afb157-de7a-4512-97f3-b67a70ba2798",
     "showTitle": true,
     "title": "HANDLING FEE"
    }
   },
   "outputs": [],
   "source": [
    "# handling = spark.sql(\n",
    "#     \"\"\"\n",
    "# WITH temp as (\n",
    "#     SELECT\n",
    "#     s.FILE_NUMBER as Expediente,\n",
    "#     CONCAT(YEAR(s.REFERENCE_DATE),'_',MONTH(s.REFERENCE_DATE)) as Mes_anio,\n",
    "#     --s.ORDER_NUMBER as Pedido,\n",
    "#     SUM(s.NET_SOLD_TKT_AMT_ITX) as Handling_fee\n",
    "#     FROM sales_vw s\n",
    "#     WHERE --YEAR(s.REFERENCE_DATE) >= 2022\n",
    "#         s.T_OPERATION_TYPE IN ('SALE','REFUND')\n",
    "#         AND s.T_OPERATION_KIND IN ('DELIVERY_OVERHEADS','ORDER_OVERHEADS')--'COMPOSED_PRODUCT' \n",
    "#     --\t\tAND s.PRODUCT_CODE != 'CAN_INS'\n",
    "#         --AND s.product_family IN ('Paquete','Competición','Evento','Paquetes',\n",
    "#             --'Pases con fecha y hora','Pase de visita','Pases sin fecha') \n",
    "#     GROUP BY\n",
    "#     s.FILE_NUMBER,s.REFERENCE_DATE\n",
    "# )\n",
    "\n",
    "# SELECT\n",
    "# CONCAT(Expediente,'|',Mes_anio) as Key_handling,\n",
    "# SUM(Handling_fee) Handling_fee\n",
    "# FROM temp\n",
    "# GROUP BY Expediente,Mes_anio\n",
    "# ORDER BY Expediente,Mes_anio\n",
    "# \"\"\"\n",
    "# )\n",
    "\n",
    "# print(handling.count())\n",
    "# display(handling.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f208d3e-2b5d-4f8d-925a-b0bebe0a7f63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # expedientes con mas de un numero pedido para handling:\n",
    "# from pyspark.sql import Window\n",
    "\n",
    "# # Agregar una columna que indique si es duplicado o no\n",
    "# window_spec = Window().partitionBy(\"Expediente\").orderBy(\"Expediente\")\n",
    "\n",
    "# df_spark = handling.withColumn(\"es_duplicado\", count(\"Expediente\").over(window_spec) > 1)\n",
    "\n",
    "# # Filtrar solo las filas duplicadas\n",
    "# duplicados = df_spark.filter(\"es_duplicado\")#.select('Evento').distinct()\n",
    "# #duplicados = duplicados.withColumn('Evento_N', lit('Hola'))\n",
    "# # DF = df3.toPandas()\n",
    "\n",
    "# # # Verificar duplicados en la columna 'llave_identificadora'\n",
    "# # duplicados = DF[DF.duplicated(subset=['Key_evento'], keep=False)]\n",
    "\n",
    "# #df_spark = spark.createDataFrame(duplicados)\n",
    "# display(duplicados)\n",
    "# print(duplicados.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62327b17-6634-47f7-ab05-7c762d2a5a39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # refund = duplicados.select('Expediente','Handling_fee').\\\n",
    "# #     groupBy(\"Expediente\").sum(\"Handling_fee\").\\\n",
    "# #         filter(col('sum(Handling_fee)') != 0)\n",
    "\n",
    "# # display(refund)\n",
    "# # print(refund.count())\n",
    "\n",
    "# handling = sales.filter(col('T_OPERATION_KIND').isin('DELIVERY_OVERHEADS','ORDER_OVERHEADS')).\\\n",
    "#     filter(col('T_OPERATION_TYPE').isin('SALE','REFUND')).\\\n",
    "#     select('FILE_NUMBER','NET_SOLD_TKT_AMT_ITX').\\\n",
    "#     groupBy(\"FILE_NUMBER\").\\\n",
    "#         agg(sum(\"NET_SOLD_TKT_AMT_ITX\").alias(\"Handling_fee\")).\\\n",
    "#     orderBy(desc(\"Handling_fee\"))\n",
    "\n",
    "# #display(handling)\n",
    "# print(handling.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "225fbf93-d530-438d-b0a5-7b9268286c9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Impresion\n",
    "# # Guarda el DataFrame en formato Parquet en Azure Blob Storage\n",
    "# inpath = \"abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/gold/TU_BOLETA/Handling_fee/\"\n",
    "# # 2023_fcf\n",
    "# join_df_coalesced = handling.coalesce(1)\n",
    "# join_df_coalesced.write.mode(\"overwrite\").parquet(inpath)\n",
    "\n",
    "# #cambio de nombre\n",
    "# files=dbutils.fs.ls(inpath)\n",
    "# output_file= [x for x in files if x.name.startswith(\"part-\")]\n",
    "\n",
    "# # EN la misma carpeta de origen\n",
    "# dbutils.fs.mv(output_file[0].path, f\"{inpath}/Ds_G_handling.parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4403015283088577,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Tuboleta_Audiencias_perfiles",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
