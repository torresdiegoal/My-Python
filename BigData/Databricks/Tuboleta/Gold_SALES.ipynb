{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35216cd4-4db0-4080-9654-64b2d643247d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "   ---------------------------------------- 0.0/235.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/235.5 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/235.5 kB 487.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 225.3/235.5 kB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 225.3/235.5 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 235.5/235.5 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cdc4422-3a88-4f96-be73-49b23bd50b8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,count,desc,sum,to_date,date_format,concat_ws,format_string,to_timestamp,year,when,lit,initcap,month,upper,lower,initcap,trim,regexp_replace\n",
    "from pyspark.sql import Window\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "\n",
    "# Crear una SparkSession\n",
    "spark = SparkSession.builder.appName(\"AzureSQL\").getOrCreate()\n",
    "\n",
    "## Credenciales BD\n",
    "inpath = \"C:/Users/diego.torres/OneDrive/Datasets/Tuboleta/Credenciales.txt\"\n",
    "keys = pd.read_csv(inpath, sep = ',')\n",
    "display(keys)\n",
    "\n",
    "# Creo variables para cada fila del DataFrame que contiene las credenciales de la bd\n",
    "for index, row in keys.iterrows():\n",
    "    variable_name = row['key']\n",
    "    variable_value = row['value']\n",
    "    globals()[variable_name] = variable_value\n",
    "\n",
    "# Configurar las propiedades de la conexión\n",
    "jdbc_url = f\"jdbc:sqlserver://{jdbc_hostname}:1433;database={jdbc_database_datamart}\"\n",
    "jdbc_properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": driver\n",
    "}\n",
    "\n",
    "# Conexion al Blob\n",
    "spark.conf.set(clave_blob, access_key_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcc8d74b-e8b0-411f-822b-ac46d8b66ee4",
     "showTitle": true,
     "title": "FUNCIONES"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "\n",
    "\n",
    "###############################################\n",
    "####  AJUSTE CIUDAD VENUE\n",
    "\n",
    "# Limpieza de texto\n",
    "def limpiar_texto(texto):\n",
    "    if isinstance(texto, str):  # Comprueba si la dirección es una cadena de texto\n",
    "        # Mayuscula\n",
    "        texto_limpio = str.upper(texto)\n",
    "        # Remover tildes y caracteres especiales de entonacion\n",
    "        texto_limpio = unidecode(texto_limpio)\n",
    "        return texto_limpio\n",
    "    else:\n",
    "        return texto\n",
    "    \n",
    "# Register User Defined Function (UDF) con Spark\n",
    "# udf: Es una función de PySpark que se utiliza para registrar una función definida por el usuario o UDF. Esta función toma dos argumentos: la función definida por el usuario y el tipo de datos de salida del UDF.\n",
    "# StringType() Especifica el tipo de datos de salida del UDF.\n",
    "# Si no registras tu función como un UDF, no podrás aplicarla directamente a una columna en un DataFrame de PySpark\n",
    "limpiar_texto = udf(limpiar_texto, StringType())\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "####  FUNCION QUE REEMPLAZA ALGUNAS VERSIONES DE CIUDAD\n",
    "\n",
    "def replace_cities_aliases(ciudad):\n",
    "    if isinstance(ciudad, str):  # Comprueba si la dirección es una cadena de texto\n",
    "        bogota_aliases = ['BOTOTA','BTA','BOGTA','BOG','BOOGTA','BGTA','KENEDY','BGA','CUNDINAMARCA','BGOOTA','BOOTA','BOFOTA','BOGITA','CEDRITOS','BOBOTA','NOGOTA','BOGATA','BOHOTA','BOGPTA','CIUDAD VERDE','CIUDADVERDE','TINTAL','TUSAQUILLO','VEINTE DE JULIO','USAQUEN','BGOTA','BOGOT','HAYUELOS','SALITRE','KENNEDY','PUENTEARANDA','LA CANDELARIA','LOS MARTIRES','BARRIOS UNIDOS','CHAPINERO','DOCE DE OCTUBRE','TEUSAQUILLO','LA CANDELARIA','CANDELARIA','SUBA','CENTRO HISTORICO','CHICO','SAN DIEGO','CENTRO','SAN PATRICIO','SANTA FE']\n",
    "        barranquilla_aliases = ['ATLANTICO','BAQ','BQLLLA','BQ','BQA','BQLLA','BARRAQNUILLA','BARRANUILLA','BQUILLLA','ER']\n",
    "        cali_aliases = ['VALLE DEL CAUCA','PARQUE DEL INGENIO','GBV','SAN NICOLAS','SANTA TERESITA']\n",
    "        cartagena_aliases = ['CTG','GETSEMANI','130001','CALLE DEL SANTISIMO','BARRIO SAN DIEGO']\n",
    "        ibague_aliases = ['IABGUE']\n",
    "        medellin_aliases = ['ANTIOQUIA','POBLADO','MED','MDELLIN','POBLADO MEDELLIN','ARANJUEZ','BARRIO COLOMBIA MEDELLIN','GFBF','CENTRO DE MEDELLIN','LA MACARENA']\n",
    "        pereira_aliases = ['LA BADEA','VILLA OLIMPICA']\n",
    "        cucuta_aliases = ['NORTE DE SANTANDER','LA MERCED']\n",
    "        vcencio_aliases = ['VILLAVICENCIO - META','VILAVICENCIO','BARRIO NUEVO MAIZARO'] \n",
    "        quibdo_aliases = ['QUIBDO-CHOO'] \n",
    "        manizales_aliases = ['SDVF','CHIPRE','PALERMO'] \n",
    "        chia_aliases = ['VSDFVS','CENTRO CHIA']\n",
    "        santa_marta_aliases = ['GBFG'] \n",
    "        bmanga_aliases = ['HNB','CALLE CIEN','GIRON','INDUSTRIAL GIRON'] \n",
    "        neiva_aliases = ['VILLA OLIMPICA'] \n",
    "        valledupar_aliases = ['VALLEUPAR'] \n",
    "        if ciudad in bogota_aliases:\n",
    "            return 'BOGOTA'\n",
    "        elif ciudad in barranquilla_aliases:\n",
    "            return 'BARRANQUILLA'\n",
    "        elif ciudad in cali_aliases:\n",
    "            return 'CALI'\n",
    "        elif ciudad in cartagena_aliases:\n",
    "            return 'CARTAGENA'\n",
    "        elif ciudad in medellin_aliases:\n",
    "            return 'MEDELLIN'\n",
    "        elif ciudad in ibague_aliases:\n",
    "            return 'IBAGUE'\n",
    "        elif ciudad in pereira_aliases:\n",
    "            return 'PEREIRA'\n",
    "        elif ciudad in cucuta_aliases:\n",
    "            return 'CUCUTA'\n",
    "        elif ciudad in vcencio_aliases:\n",
    "            return 'VILLAVICENCIO'\n",
    "        elif ciudad in quibdo_aliases:\n",
    "            return 'QUIBDO'\n",
    "        elif ciudad in manizales_aliases:\n",
    "            return 'MANIZALES'\n",
    "        elif ciudad in chia_aliases:\n",
    "            return 'CHIA'\n",
    "        elif ciudad in santa_marta_aliases:\n",
    "            return 'SANTA MARTA'\n",
    "        elif ciudad in bmanga_aliases:\n",
    "            return 'BUCARAMANGA'\n",
    "        elif ciudad in neiva_aliases:\n",
    "            return 'NEIVA'\n",
    "        elif ciudad in valledupar_aliases:\n",
    "            return 'VALLEDUPAR'\n",
    "        else:\n",
    "            return ciudad\n",
    "    else:\n",
    "        return ciudad\n",
    "    \n",
    "replace_cities_aliases = udf(replace_cities_aliases, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da803675-2782-4c4c-9290-2c84149dc9b4",
     "showTitle": true,
     "title": "IMPORTE DE LOS AÑOS DE VENTAS"
    }
   },
   "outputs": [],
   "source": [
    "## Ventas 2019\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/sales/2019/Ds_S3_Sales_2019.parquet/'\n",
    "sales_2019 = spark.read.parquet(inpath)\n",
    "#print(sales_2019.count())\n",
    "#sales_2019.createOrReplaceTempView(\"sales_2019_vw\")\n",
    "\n",
    "## Ventas 2020\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/sales/2020/Ds_S3_Sales_2020.parquet/'\n",
    "sales_2020 = spark.read.parquet(inpath)\n",
    "#print(sales_2020.count())\n",
    "#sales_2020.createOrReplaceTempView(\"sales_2020_vw\")\n",
    "\n",
    "## Ventas 2021\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/sales/2021/Ds_S3_Sales_2021.parquet/'\n",
    "sales_2021 = spark.read.parquet(inpath)\n",
    "#print(sales_2021.count())\n",
    "#sales_2021.createOrReplaceTempView(\"sales_2021_vw\")\n",
    "\n",
    "## Ventas 2022\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/sales/2022/Ds_S3_Sales_2022.parquet/'\n",
    "sales_2022 = spark.read.parquet(inpath)\n",
    "#print(sales_2022.count()) \n",
    "#sales_2022.createOrReplaceTempView(\"sales_2022_vw\")\n",
    "\n",
    "# Ventas 2023\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/sales/2023/Ds_S3_Sales_2023.parquet/'\n",
    "sales_2023 = spark.read.parquet(inpath)\n",
    "#print(sales_2023.count())\n",
    "#sales_2023.createOrReplaceTempView(\"sales_2023_vw\")\n",
    "\n",
    "\n",
    "## Ventas 2024\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/sales/2024/S3/Ds_S3_Sales_2024.parquet/'\n",
    "sales_2024 = spark.read.parquet(inpath)\n",
    "#print(sales_2024.count())\n",
    "#sales_2023.createOrReplaceTempView(\"sales_2023_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f18e1953-42ad-43b8-842f-407cbb1d07d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columnas = [\"ORGANIZATION\",\"T_ORGANIZ_ID\",\"PRODUCT_FAMILY\",\"PRODUCT\",\"INVOICE_CONTACT_FMT_NAME\", \"T_PRODUCT_ID\",\"LICENCE_NUMBER\",\"PRODUCT_STATE\",\"SITE\",\"SEASON\",\"TOPIC\",\"SUB_TOPIC\",\"REFERENCE_DATE\",\"ORDER_DATE_MM_YY\",\"PRODUCT_DATE_TIME\",\"PRODUCT_TIME\",\"FILE_NUMBER\",\"T_ORDER_CONTACT_ID\",\"PROMOTION\",\"AUDIENCE_CATEGORY\",\"AUDIENCE_SUB_CATEGORY\",\"LOGICAL_SEAT_CATEGORY\",\"PERFORMANCE_QUOTA\",\"SALES_CHANNEL\",\"SALES_CHANNEL_N\",\"SALES_CHANNEL_TYPE\",\"UNIT_AMT_ITX\",\"NET_SOLD_T_QTY\",\"NET_SOLD_C_QTY\",\"NET_SOLD_TKT_AMT_ITX\",\"TEAM_1\",\"TEAM_2\",\"ORDER_NUMBER\",\"CHARGE_AMT_ITX\",\"ORDER_CREATION_DATE\",\"ORDER_CONTACT_NUMBER\",\"ORDER_DATE\",\"PRODUCT_DATE\",\"CHARGE_AMT_ETX\",\"total_unit_amt_itx\",\"DN_QUOTA\",\"T_INVOICE_CONTACT_ID\",\"File_state\",\"t_file_state\",\"ORDER_QUANTITY\",\"T_OPERATION_TYPE\",\"PRODUCT_CODE\",\"t_operation_kind\",\"parent_product\",\"INSURED\",\"INSURED_AMT_ITX\",\"T_PERFORMANCE_ID\",\"T_SITE_ID\",\"NET_SOLD_AMT_ITX\",\"PRODUCT_EXTERNAL_NAME\",\"T_SEASON_ID\",\"ACTIVITY\",\"T_ACTIVITY_ID\",\"T_PRODUCT_FAMILY_ID\",\"PRODUCT_TYPE\",\"T_PRODUCT_TYPE_ID\",\"T_FAMILY_TYPE\",\"T_FAMILY_SUB_TYPE\",\"T_TOPIC_ID\",\"T_SHIPMENTFEE_ID\",\"TOWN\",\"TOPIC_KEY\",\"TOPIC_NW\",\"SUB_TOPIC_NW\",\"INSURED_AMT_ETX\"]\n",
    "\n",
    "## Seleccion de columnas ###\n",
    "sales_2019 = sales_2019.select(columnas)\n",
    "sales_2020 = sales_2020.select(columnas)\n",
    "sales_2021 = sales_2021.select(columnas)\n",
    "sales_2022 = sales_2022.select(columnas)\n",
    "sales_2023 = sales_2023.select(columnas)\n",
    "sales_2024 = sales_2024.select(columnas)\n",
    "\n",
    "\n",
    "# EN 2024 tengo 2023 incluido\n",
    "#sales = sales_2019.union(sales_2020).union(sales_2021).union(sales_2022).union(sales_2023).union(sales_2024)\n",
    "sales = sales_2019.union(sales_2020).union(sales_2021).union(sales_2022).union(sales_2023).union(sales_2024)\n",
    "\n",
    "#print(sales.count())\n",
    "sales.createOrReplaceTempView(\"sales_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f799437-a702-44fd-b99d-805d6b63e430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tmp = sales.select('LOGICAL_SEAT_CATEGORY','T_PRODUCT_ID','PRODUCT_FAMILY').filter(col('T_PRODUCT_ID') == 10229437245287).distinct()\n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "500481de-5434-491a-bd17-cc3792561137",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sales' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m concat_eventos \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT_PRODUCT_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT_PERFORMANCE_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m concat_hfee \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFILE_NUMBER\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT_PRODUCT_ID\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT_PERFORMANCE_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m sales \u001b[38;5;241m=\u001b[39m \u001b[43msales\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOPIC_NW\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOPIC_N\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m      9\u001b[0m     withColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUB_TOPIC_NW\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUB_TOPIC_N\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m     10\u001b[0m     withColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRODUCT\u001b[39m\u001b[38;5;124m\"\u001b[39m, limpiar_texto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRODUCT\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m     11\u001b[0m     withColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT_PRODUCT_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, format_string(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%.0f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT_PRODUCT_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m     12\u001b[0m     withColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT_PERFORMANCE_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, format_string(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%.0f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT_PERFORMANCE_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m     13\u001b[0m     withColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFecha_evento\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_timestamp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRODUCT_DATE_TIME\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMM/dd/yyyy HH:mm:ss\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m     14\u001b[0m     withColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate_evento\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_date(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFecha_evento\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m     15\u001b[0m     withColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHora_evento\u001b[39m\u001b[38;5;124m\"\u001b[39m, date_format(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRODUCT_DATE_TIME\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHH:mm:ss\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m     16\u001b[0m     withColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey_evento\u001b[39m\u001b[38;5;124m\"\u001b[39m, concat_ws(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mconcat_eventos))\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m     17\u001b[0m     withColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey_handling\u001b[39m\u001b[38;5;124m\"\u001b[39m, concat_ws(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mconcat_hfee))\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m     18\u001b[0m     withColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOGICAL_SEAT_CATEGORY\u001b[39m\u001b[38;5;124m\"\u001b[39m, upper(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOGICAL_SEAT_CATEGORY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m     19\u001b[0m     withColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOGICAL_SEAT_CATEGORY\u001b[39m\u001b[38;5;124m\"\u001b[39m, regexp_replace(trim(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOGICAL_SEAT_CATEGORY\u001b[39m\u001b[38;5;124m\"\u001b[39m)), \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+$\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m     20\u001b[0m     withColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOWN_N\u001b[39m\u001b[38;5;124m\"\u001b[39m, limpiar_texto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOWN\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m     21\u001b[0m     withColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOWN_N\u001b[39m\u001b[38;5;124m\"\u001b[39m, replace_cities_aliases(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOWN_N\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m#withColumn(\"LOCALIDAD_GP\", localidad_agrupada(col(\"Direccion_new\")))\u001b[39;00m\n\u001b[0;32m     24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalario\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalario\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^At least\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sales' is not defined"
     ]
    }
   ],
   "source": [
    "# TRANSFORMACIONES\n",
    "# columns_to_concat = [\"T_PRODUCT_ID\",'PRODUCT_DATE', \"PRODUCT_TIME\"]\n",
    "concat_aforo_pases_visita = [\"T_PRODUCT_ID\",'PRODUCT_DATE', \"PRODUCT_TIME\"]\n",
    "concat_aforo_eventos = [\"T_PRODUCT_ID\",'T_PERFORMANCE_ID','LOGICAL_SEAT_CATEGORY']\n",
    "concat_eventos = [\"T_PRODUCT_ID\",'T_PERFORMANCE_ID']\n",
    "concat_hfee = ['FILE_NUMBER','T_PRODUCT_ID','T_PERFORMANCE_ID']\n",
    "\n",
    "sales = sales.withColumnRenamed(\"TOPIC_NW\", \"TOPIC_N\").\\\n",
    "    withColumnRenamed(\"SUB_TOPIC_NW\", \"SUB_TOPIC_N\").\\\n",
    "    withColumn(\"PRODUCT\", limpiar_texto(\"PRODUCT\")).\\\n",
    "    withColumn(\"T_PRODUCT_ID\", format_string(\"%.0f\", col(\"T_PRODUCT_ID\"))).\\\n",
    "    withColumn(\"T_PERFORMANCE_ID\", format_string(\"%.0f\", col(\"T_PERFORMANCE_ID\"))).\\\n",
    "    withColumn(\"Fecha_evento\", to_timestamp('PRODUCT_DATE_TIME', \"MM/dd/yyyy HH:mm:ss\")).\\\n",
    "    withColumn(\"Date_evento\", to_date(\"Fecha_evento\")).\\\n",
    "    withColumn(\"Hora_evento\", date_format(\"PRODUCT_DATE_TIME\", \"HH:mm:ss\")).\\\n",
    "    withColumn(\"Key_evento\", concat_ws(\"|\", *concat_eventos)).\\\n",
    "    withColumn(\"Key_handling\", concat_ws(\"|\", *concat_hfee)).\\\n",
    "    withColumn(\"LOGICAL_SEAT_CATEGORY\", upper(\"LOGICAL_SEAT_CATEGORY\")).\\\n",
    "    withColumn(\"LOGICAL_SEAT_CATEGORY\", regexp_replace(trim(col(\"LOGICAL_SEAT_CATEGORY\")), r'^\\s+|\\s+$', '')).\\\n",
    "    withColumn(\"TOWN_N\", limpiar_texto(\"TOWN\")).\\\n",
    "    withColumn(\"TOWN_N\", replace_cities_aliases(col('TOWN_N')))\n",
    "    #withColumn(\"LOCALIDAD_GP\", localidad_agrupada(col(\"Direccion_new\")))\n",
    "\n",
    "\n",
    "# Aplicación de unificacion de pases de visita y pases con fecha y hora\n",
    "sales = sales.withColumn(\"PRODUCT_FAMILY\",\n",
    "                   when(sales[\"PRODUCT_FAMILY\"].isin(\"Pases con fecha y hora\"),\n",
    "                        lit('Pase de visita')).otherwise(sales[\"PRODUCT_FAMILY\"]))\n",
    "\n",
    "# # Aplicación de la lógica de reemplazo en el DataFrame df_follwup\n",
    "sales = sales.withColumn(\"Key_aforo\",\n",
    "                   when(sales[\"PRODUCT_FAMILY\"].isin(\"Pase de visita\"),\n",
    "                        concat_ws(\"|\", *concat_aforo_pases_visita)).otherwise(concat_ws(\"|\", *concat_aforo_eventos)))   \n",
    "\n",
    "sales.createOrReplaceTempView(\"sales_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "947f7dc1-6be1-42b9-ac85-fd8bcb08aae6",
     "showTitle": true,
     "title": "HANDLING FEE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4272655\n"
     ]
    }
   ],
   "source": [
    "handling = spark.sql(\n",
    "    \"\"\"\n",
    "WITH hfee as (\n",
    "    SELECT\n",
    "    s.FILE_NUMBER as Expediente,\n",
    "    SUM(s.NET_SOLD_TKT_AMT_ITX) as Handling_fee\n",
    "    FROM sales_vw s\n",
    "    WHERE s.T_OPERATION_TYPE IN ('SALE','REFUND')\n",
    "        AND s.T_OPERATION_KIND IN ('DELIVERY_OVERHEADS','ORDER_OVERHEADS')--'COMPOSED_PRODUCT' \n",
    "    GROUP BY s.FILE_NUMBER),\n",
    "\n",
    "eventos as (\n",
    "    SELECT DISTINCT      \n",
    "      FILE_NUMBER as Expediente,\n",
    "      Key_handling\n",
    "    FROM sales_vw \n",
    "    WHERE product_family IN ('Competición','Evento','Pase de visita')\n",
    "        --AND s.PRODUCT_CODE != 'CAN_INS'\n",
    "        AND UPPER(PRODUCT) NOT LIKE '%PRUEBA%'\n",
    "        AND UPPER(PRODUCT) NOT LIKE '%SUITE%'\n",
    "        AND UPPER(PRODUCT) NOT LIKE '%PARQUEADERO%'\n",
    "        AND UPPER(PRODUCT) NOT LIKE '%PREIMPRESO%'\n",
    "        AND UPPER(PRODUCT) NOT LIKE '%ACREDITAC%'\n",
    "        AND UPPER(PRODUCT) NOT LIKE '%REDENCI%'\n",
    "        AND UPPER(PRODUCT) NOT LIKE '%ZONAS VIP%'\n",
    "        AND UPPER(PRODUCT) NOT LIKE '%PCR%'\n",
    "        AND UPPER(PRODUCT) NOT LIKE '%GENOS%'\n",
    "        AND UPPER(PRODUCT) NOT LIKE '%TUBOLETA TE LLEVA%'\n",
    "        AND UPPER(PRODUCT) NOT LIKE '%CONCIERGE%'\n",
    "        AND UPPER(PRODUCT) NOT LIKE '%VMOVE%'),\n",
    "\n",
    "n_eventos as (\n",
    "    SELECT\n",
    "    Expediente,\n",
    "    count(Key_handling) as N_eventos_x_expediente\n",
    "    FROM eventos\n",
    "    GROUP BY Expediente),\n",
    "\n",
    "hfee_X_evento as (\n",
    "    SELECT\n",
    "    a.Expediente,\n",
    "    a.N_eventos_x_expediente,\n",
    "    b.Handling_fee,\n",
    "    b.Handling_fee/a.N_eventos_x_expediente as Handling_fee_x_evento\n",
    "    FROM n_eventos a\n",
    "    LEFT JOIN hfee b\n",
    "    ON (a.Expediente = b.Expediente)\n",
    "    WHERE b.Handling_fee != 0 AND b.Handling_fee is not null )\n",
    "\n",
    "SELECT \n",
    "  a.Key_handling,\n",
    "  b.Handling_fee_x_evento\n",
    "FROM eventos a\n",
    "LEFT JOIN hfee_X_evento b\n",
    "  ON (a.Expediente = b.Expediente)\n",
    "WHERE (b.Handling_fee_x_evento != 0 AND b.Handling_fee_x_evento is not null)\n",
    "ORDER BY Key_handling\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(handling.count())\n",
    "#display(handling.limit(1000))\n",
    "handling.createOrReplaceTempView(\"handling_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66221367-4522-48ca-9771-f4644c427090",
     "showTitle": true,
     "title": "VALIDACION DEL HFEE"
    }
   },
   "outputs": [],
   "source": [
    "# handling = spark.sql(\n",
    "#     \"\"\"\n",
    "# WITH hfee as (\n",
    "#     SELECT\n",
    "#     s.FILE_NUMBER as Expediente,\n",
    "#     SUM(s.NET_SOLD_TKT_AMT_ITX) as Handling_fee\n",
    "#     FROM sales_vw s\n",
    "#     WHERE s.T_OPERATION_TYPE IN ('SALE','REFUND')\n",
    "#         AND s.T_OPERATION_KIND IN ('DELIVERY_OVERHEADS','ORDER_OVERHEADS')--'COMPOSED_PRODUCT' \n",
    "#     GROUP BY s.FILE_NUMBER),\n",
    "\n",
    "# eventos as (\n",
    "#     SELECT DISTINCT\n",
    "#     s.FILE_NUMBER as Expediente,\n",
    "#     s.Key_evento\n",
    "#     FROM sales_vw s\n",
    "#     WHERE s.product_family IN ('Competición','Evento','Pase de visita')\n",
    "#         --AND s.PRODUCT_CODE != 'CAN_INS'\n",
    "#         AND UPPER(s.PRODUCT) NOT LIKE '%PRUEBA%'\n",
    "#         AND UPPER(s.PRODUCT) NOT LIKE '%SUITE%'\n",
    "#         AND UPPER(s.PRODUCT) NOT LIKE '%PARQUEADERO%'\n",
    "#         AND UPPER(s.PRODUCT) NOT LIKE '%PREIMPRESO%'\n",
    "#         AND UPPER(s.PRODUCT) NOT LIKE '%ACREDITAC%'\n",
    "#         AND UPPER(s.PRODUCT) NOT LIKE '%REDENCI%'\n",
    "#         AND UPPER(s.PRODUCT) NOT LIKE '%ZONAS VIP%'\n",
    "#         AND UPPER(s.PRODUCT) NOT LIKE '%PCR%'\n",
    "#         AND UPPER(s.PRODUCT) NOT LIKE '%GENOS%'\n",
    "#         AND UPPER(s.PRODUCT) NOT LIKE '%TUBOLETA TE LLEVA%'\n",
    "#         AND UPPER(s.PRODUCT) NOT LIKE '%CONCIERGE%'\n",
    "#         AND UPPER(s.PRODUCT) NOT LIKE '%VMOVE%'),\n",
    "\n",
    "# n_eventos as (\n",
    "#     SELECT\n",
    "#     Expediente,\n",
    "#     count(Key_evento) as N_eventos_x_expediente\n",
    "#     FROM eventos\n",
    "#     GROUP BY Expediente)\n",
    "\n",
    "# SELECT\n",
    "# a.Expediente,\n",
    "# a.N_eventos_x_expediente,\n",
    "# b.Handling_fee,\n",
    "# b.Handling_fee/a.N_eventos_x_expediente as Handling_fee_x_evento\n",
    "# FROM n_eventos a\n",
    "# LEFT JOIN hfee b\n",
    "#   ON (a.Expediente = b.Expediente)\n",
    "# WHERE (b.Handling_fee != 0 AND b.Handling_fee is not null)\n",
    "#     AND a.Expediente = '10000096'\n",
    "\n",
    "\n",
    "# \"\"\")\n",
    "\n",
    "# #print(handling.count())\n",
    "# display(handling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e935c016-091e-4c75-9131-e9803757798e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tmp = sales.select('FILE_NUMBER','PRODUCT','product_family','REFERENCE_DATE','PRODUCT_DATE_TIME').\\\n",
    "    filter(col('FILE_NUMBER') == '10000096').distinct()\n",
    "display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf759847-10ff-4bd8-be75-5b9c53f3b064",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tmp = sales.\\\n",
    "#     select('TOWN','SITE','TOWN_V2').distinct()\n",
    "# # #tmp = sales.filter(col('T_PRODUCT_ID') == '10229196910867') #eladio cali\n",
    "# # #print(tmp.count())\n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60da79ed-9652-477b-87ca-e75d2503aedd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # tmp = sales.select('t_file_state').distinct()\n",
    "# # display(tmp)\n",
    "\n",
    "# tmp = sales.select('T_PRODUCT_ID','TOPIC').filter(col('T_PRODUCT_ID') == '10229196910867').distinct() #eladio cali\n",
    "# #print(tmp.count())\n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f65492-5b59-4962-9038-dc329496a9a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tmp = sales.select('T_PERFORMANCE_ID','TEAM_1','TEAM_2').\\\n",
    "#     filter(col('T_PERFORMANCE_ID') == '10229481277847').distinct()\n",
    "\n",
    "# #display(tmp.limit(100))\n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a6d1fc-cf96-4b43-8eff-8065fa5d88d3",
     "showTitle": true,
     "title": "IMPORTE DE MAESTROS"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1839039\n",
      "25537\n"
     ]
    }
   ],
   "source": [
    "## TABLA AFOROS ACTUALIZADA\n",
    "inpath = \"abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/Follwup/Ds_S_Follwup.parquet\"\n",
    "df_aforos = spark.read.parquet(inpath)\n",
    "print(df_aforos.count())\n",
    "\n",
    "#df_aforos.createOrReplaceTempView(\"aforos_vw\")\n",
    "\n",
    "## MAESTRA TOPICS\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/gold/TU_BOLETA/Maestro/Maestro_Tema_Subtema/'\n",
    "maestro = spark.read.parquet(inpath)\n",
    "\n",
    "print(maestro.count())\n",
    "#display(maestro.limit(100))\n",
    "\n",
    "\n",
    "## TABLA AFOROS ACTUALIZADA\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/Configuration/Ds_S_Configuration.parquet/'\n",
    "configuration = spark.read.parquet(inpath)\n",
    "configuration.createOrReplaceTempView(\"configuration_vw\")\n",
    "#print(configuration.count())\n",
    "\n",
    "\n",
    "## ciudad TOWN\n",
    "blob_storage_path = \"abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/Ciudades/Ciudades.csv/\"\n",
    "ciudad = spark.read.csv(blob_storage_path,header=True, inferSchema=True, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a2ac7e-5cfe-4abc-bc7c-6574d5f84529",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Key_evento</th><th>T_PRODUCT_ID</th><th>T_PERFORMANCE_ID</th><th>PRODUCT</th><th>TEAM_1</th><th>TEAM_2</th><th>MATCH_DAY</th><th>TOPIC</th><th>Edad_minima</th><th>NIT</th><th>Contrato</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Key_evento",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "T_PRODUCT_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "T_PERFORMANCE_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PRODUCT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TEAM_1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TEAM_2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "MATCH_DAY",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TOPIC",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Edad_minima",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "NIT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Contrato",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tmp = df_aforos.filter(col('Key_evento_').like('%10229437245287%'))\n",
    "# display(tmp)\n",
    "\n",
    "tmp = configuration.filter(col('Key_evento').like('%10229649694546%'))\n",
    "display(tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57ff0da2-2cf3-451b-8cce-5347a0e680c0",
     "showTitle": true,
     "title": "Ajuste TOWN"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import concat_ws\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# # Columnas a concatenar\n",
    "# columns_to_concat = [\"ORGANIZATION\", \"TOWN\", \"SITE\"]\n",
    "\n",
    "# # Utiliza la función concat_ws para concatenar las columnas con un separador (en este caso, \"\")\n",
    "# sales_2 = sales.withColumn(\"concatenated_column\", concat_ws(\"\", *columns_to_concat))\n",
    "\n",
    "\n",
    "# # Realiza la unión basada en las columnas \"concatenated_column\" y \"CONCATENADO\"\n",
    "# joined_df = sales_2.join(ciudad, sales_2[\"concatenated_column\"] == ciudad[\"CONCATENADO\"], \"left_outer\")\n",
    "\n",
    "# # Borras las columnaas del csv\n",
    "# columns_to_drop = [\"ORGANIZATION2\",\"SITE2\", \"CONCATENADO\",\"concatenated_column\"]\n",
    "\n",
    "# # Crea un nuevo DataFrame sin las columnas especificadas\n",
    "# result_df = joined_df.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "# # Renombra la columna \"HOMOLOGA\" como \"TOWN\"\n",
    "# sales = result_df.withColumnRenamed(\"HOMOLOGA\", \"TOWN_V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d89d8b41-fc3f-43a0-98c9-e07aec89abab",
     "showTitle": true,
     "title": "Separacion de maestras"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284\n",
      "39\n",
      "271\n",
      "187\n",
      "72\n",
      "823\n",
      "322\n",
      "25537\n"
     ]
    }
   ],
   "source": [
    "# CAMPOS DE CADA MAESTRA\n",
    "venues = [\"Emplazamiento_Secutix\",'Venue_Panel']\n",
    "temas_ppto = [\"Temas_2023\",'Segmento_Presupuesto_2023']\n",
    "tipo_venue = [\"VENUE\",'Tipo_de_escenario']\n",
    "canales_venta = [\"CANAL_DE_VENTA\",'Agrupacion_Canal']\n",
    "medios_pago = [\"Medio_Pago\",'Agrupacion_Medio_Pago']\n",
    "productos_2024 = [\"Producto_2024\",'Categoria_Producto']\n",
    "promotores = [\"Promotor_Secutix\",'Promotor_Panel']\n",
    "productos_2023 = [\"Producto_2023\",'Codigo_de_producto','Sesiones','Aforo','Segmento_Presupuesto_2023_2','Tema','Subtema']\n",
    "\n",
    "# MAESTRAS\n",
    "venues = maestro.select(venues).\\\n",
    "    filter(col('Emplazamiento_Secutix').isNotNull())\n",
    "print(venues.count())\n",
    "#display(venues)\n",
    "\n",
    "temas_ppto = maestro.select(temas_ppto).\\\n",
    "    filter(col('Temas_2023').isNotNull())\n",
    "print(temas_ppto.count())\n",
    "\n",
    "tipo_venue = maestro.select(tipo_venue).\\\n",
    "    filter(col('VENUE').isNotNull())\n",
    "print(tipo_venue.count())\n",
    "#display(tipo_venue)\n",
    "\n",
    "canales_venta = maestro.select(canales_venta).\\\n",
    "    filter(col('CANAL_DE_VENTA').isNotNull())\n",
    "print(canales_venta.count())\n",
    "\n",
    "medios_pago = maestro.select(medios_pago).\\\n",
    "    filter(col('Medio_Pago').isNotNull())\n",
    "print(medios_pago.count())\n",
    "\n",
    "productos_2024 = maestro.select(productos_2024).\\\n",
    "    filter(col('Producto_2024').isNotNull())\n",
    "print(productos_2024.count())\n",
    "\n",
    "promotores = maestro.select(promotores).\\\n",
    "    filter(col('Promotor_Secutix').isNotNull())\n",
    "print(promotores.count())\n",
    "\n",
    "productos_2023 = maestro.select(productos_2023).\\\n",
    "    filter(col('Producto_2023').isNotNull())\n",
    "print(productos_2023.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7981e59a-9406-4841-b256-564d04e0726a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AJUSTE DE AFORO\n",
    "sales = sales.join(df_aforos['Key_evento_',\"Nuevo_aforo\"],\\\n",
    "    sales[\"Key_aforo\"] == df_aforos[\"Key_evento_\"],\\\n",
    "    \"left_outer\")\n",
    "\n",
    "\n",
    "# AJUSTE DE VENUE\n",
    "sales = sales.join(venues,\\\n",
    "    sales[\"SITE\"] == venues[\"Emplazamiento_Secutix\"],\\\n",
    "    \"left_outer\").\\\n",
    "    withColumnRenamed(\"Venue_Panel\", \"VENUE_N\").\\\n",
    "    withColumn(\"VENUE_N\", initcap(\"VENUE_N\"))\n",
    "\n",
    "# AJUSTE DE TIPO VENUE\n",
    "sales = sales.join(tipo_venue,\\\n",
    "    sales[\"VENUE_N\"] == tipo_venue[\"VENUE\"],\\\n",
    "    \"left_outer\").\\\n",
    "    withColumnRenamed(\"Tipo_de_escenario\", \"Tipo_venue\")\n",
    "\n",
    "# AJUSTE DE CANALES DE VENTA\n",
    "# sales = sales.join(canales_venta,\\\n",
    "#     sales[\"VENUE_N\"] == canales_venta[\"CANAL_DE_VENTA\"],\\\n",
    "#     \"left_outer\").\\\n",
    "#     withColumnRenamed(\"Tipo_de_escenario\", \"Tipo_venue\")\n",
    "\n",
    "\n",
    "\n",
    "# # # Crea un nuevo DataFrame sin las columnas especificadas\n",
    "columns_to_drop = [\"Key_evento_\",'Emplazamiento_Secutix','VENUE']\n",
    "sales = sales.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "#sales = sales_2021.union(sales_2022).union(sales_2023)\n",
    "\n",
    "#print(sales_2023.count())\n",
    "sales.createOrReplaceTempView(\"sales_vw\")\n",
    "#print(sales.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cbabd32-eaae-4065-9e73-ec6a6020c331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ajuste de equipos de futbol visitantes por definir\n",
    "sales = spark.sql(\n",
    "    \"\"\"\n",
    "    select \n",
    "      s.*,\n",
    "      CASE \n",
    "            WHEN c.TEAM_2 IS NULL THEN s.TEAM_2\n",
    "            ELSE c.TEAM_2 \n",
    "        END AS TEAM_2_N,\n",
    "      c.NIT,\n",
    "      c.Contrato,\n",
    "      c.MATCH_DAY,\n",
    "      CASE \n",
    "            WHEN c.TOPIC IS NULL THEN s.TOPIC\n",
    "            ELSE c.TOPIC \n",
    "        END AS TOPIC_PPTO_old,\n",
    "      CASE \n",
    "            WHEN c.PRODUCT IS NULL THEN s.PRODUCT\n",
    "            ELSE c.PRODUCT \n",
    "        END as PRODUCT_N,\n",
    "      CASE \n",
    "            WHEN h.Handling_fee_x_evento IS NULL THEN 0\n",
    "            ELSE h.Handling_fee_x_evento \n",
    "        END as Hfee\n",
    "    from sales_vw s\n",
    "    left join configuration_vw c\n",
    "      on s.Key_evento = c.Key_evento\n",
    "    left join handling_vw h\n",
    "      on s.Key_handling = h.Key_handling\n",
    "    \"\"\")\n",
    "\n",
    "# AJUSTE DE TEMA\n",
    "sales = sales.join(temas_ppto,\\\n",
    "    sales[\"TOPIC_PPTO_old\"] == temas_ppto[\"Temas_2023\"],\\\n",
    "    \"left_outer\").\\\n",
    "    withColumnRenamed(\"Segmento_Presupuesto_2023\", \"TOPIC_PPTO\")\n",
    "\n",
    "\n",
    "# Borras las columnaas del csv\n",
    "columns_to_drop = [\"TOPIC_PPTO_old\",'Temas_2023']\n",
    "# Crea un nuevo DataFrame sin las columnas especificadas\n",
    "sales = sales.drop(*columns_to_drop)\n",
    "\n",
    "\n",
    "#display(sales)\n",
    "#print(sales.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77308e64-6ee2-478d-8ff9-3fc1ed9446f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tmp = sales.select('Key_evento','Key_aforo','Nuevo_aforo').filter(col('Key_evento').like('%10229437245287%')).distinct()\n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d1c0ef-61e1-4601-8cd6-0e3a023d9640",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# tmp = sales.select('T_PERFORMANCE_ID','TEAM_2','TEAM_2_N').\\\n",
    "#     filter(col('T_PERFORMANCE_ID') == '10229481277847').distinct()\n",
    "\n",
    "# #display(tmp.limit(100))\n",
    "# display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e7fe73-e266-4832-91a2-e5dd1453de8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-608797768202134>, line 6\u001b[0m\n",
       "\u001b[1;32m      3\u001b[0m inpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabfss://storagebi@tbdwhstorage01.dfs.core.windows.net/gold/Completo_ventas/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
       "\u001b[1;32m      5\u001b[0m join_df_coalesced \u001b[38;5;241m=\u001b[39m sales\u001b[38;5;241m.\u001b[39mcoalesce(\u001b[38;5;241m1\u001b[39m)\n",
       "\u001b[0;32m----> 6\u001b[0m join_df_coalesced\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(inpath)\n",
       "\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#join_df_coalesced.write.mode(\"overwrite\").orc(path_parquet_snappy)\u001b[39;00m\n",
       "\u001b[1;32m      8\u001b[0m \n",
       "\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#cambio de nombre\u001b[39;00m\n",
       "\u001b[1;32m     10\u001b[0m files\u001b[38;5;241m=\u001b[39mdbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mls(inpath)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n",
       "\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n",
       "\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n",
       "\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    190\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
       "\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
       "\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
       "\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
       "\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [COLUMN_ALREADY_EXISTS] The column `t_file_state` already exists. Consider to choose another name or rename the existing column."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\nFile \u001b[0;32m<command-608797768202134>, line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m inpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabfss://storagebi@tbdwhstorage01.dfs.core.windows.net/gold/Completo_ventas/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m join_df_coalesced \u001b[38;5;241m=\u001b[39m sales\u001b[38;5;241m.\u001b[39mcoalesce(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m join_df_coalesced\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(inpath)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#join_df_coalesced.write.mode(\"overwrite\").orc(path_parquet_snappy)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#cambio de nombre\u001b[39;00m\n\u001b[1;32m     10\u001b[0m files\u001b[38;5;241m=\u001b[39mdbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mls(inpath)\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    190\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\n\u001b[0;31mAnalysisException\u001b[0m: [COLUMN_ALREADY_EXISTS] The column `t_file_state` already exists. Consider to choose another name or rename the existing column.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [COLUMN_ALREADY_EXISTS] The column `t_file_state` already exists. Consider to choose another name or rename the existing column.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Impresion\n",
    "# Guarda el DataFrame en formato Parquet en Azure Blob Storage\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/gold/Completo_ventas/'\n",
    "\n",
    "join_df_coalesced = sales.coalesce(1)\n",
    "join_df_coalesced.write.mode(\"overwrite\").parquet(inpath)\n",
    "#join_df_coalesced.write.mode(\"overwrite\").orc(path_parquet_snappy)\n",
    "\n",
    "#cambio de nombre\n",
    "files=dbutils.fs.ls(inpath)\n",
    "output_file= [x for x in files if x.name.startswith(\"part-\")]\n",
    "\n",
    "# EN la misma carpeta de origen\n",
    "dbutils.fs.mv(output_file[0].path, f\"{inpath}/Ds_G_sales.parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SALES_gold",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
