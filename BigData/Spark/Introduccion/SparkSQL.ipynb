{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Creating the SparkSession, very important before doing any action in spark\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    appName('SparkSQL').\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read a flat file indicating to infer the schema\n",
    "db = spark.read.format('csv').\\\n",
    "    option('inferSchema','true').\\\n",
    "    option('header','true').\\\n",
    "    option('path','operations_management.csv').\\\n",
    "    load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- description: string (nullable = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- level: integer (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      " |-- line_code: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we gonna do transformation using:\n",
    "#   a) API DataFrame\n",
    "#   b) Spark SQL query    \n",
    "# 'total' is the value of the col 'industry', not a colname\n",
    "db_T1 = db.select(\"industry\",\"value\").\\\n",
    "filter((col(\"value\") > 200) & (col(\"industry\") != \"total\")).\\\n",
    "orderBy(desc(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            industry|value|\n",
      "+--------------------+-----+\n",
      "|        Construction| 6030|\n",
      "|        Construction| 5904|\n",
      "|        Construction| 5229|\n",
      "|Accommodation & f...| 5058|\n",
      "|        Construction| 4965|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db_T1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- industry: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db_T1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            industry|value|\n",
      "+--------------------+-----+\n",
      "|        Construction| 6030|\n",
      "|        Construction| 5904|\n",
      "|        Construction| 5229|\n",
      "|Accommodation & f...| 5058|\n",
      "|        Construction| 4965|\n",
      "|        Construction| 4959|\n",
      "|Accommodation & f...| 4950|\n",
      "|        Construction| 4686|\n",
      "|        Construction| 4668|\n",
      "|        Construction| 4665|\n",
      "|       Manufacturing| 4662|\n",
      "|       Manufacturing| 4632|\n",
      "|        Construction| 4575|\n",
      "|        Construction| 4566|\n",
      "|Professional, sci...| 4476|\n",
      "|Professional, sci...| 4470|\n",
      "|        Retail trade| 4434|\n",
      "|        Retail trade| 4434|\n",
      "|Accommodation & f...| 4251|\n",
      "|Accommodation & f...| 4176|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# b) Same query but using spark.sql and SQL sentences\n",
    "db_T1.createOrReplaceTempView('db_view') # need to create a view in order to be allowed to query the data\n",
    "# data in a spark view can be only showed after being queried as below\n",
    "spark.sql(\"\"\"SELECT *   \n",
    "             FROM db_view \n",
    "             WHERE value > 200 \n",
    "                AND industry != 'total' \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# b) Same query but using spark.sql and SQL sentences\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m db_T1\u001b[39m.\u001b[39;49mcreateOrReplaceTempView(\u001b[39m'\u001b[39;49m\u001b[39mdb_view\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mshow()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "# b) Same query but using spark.sql and SQL sentences\n",
    "db_T1.createOrReplaceTempView('db_view').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
