{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,count,desc,sum,to_date,date_format,concat_ws,format_string,to_timestamp,year,when,lit,month,date_sub,current_date,max,min\n",
    "from pyspark.sql import Window\n",
    "import pandas as pd\n",
    "\n",
    "# Crear una SparkSession\n",
    "spark = SparkSession.builder.appName(\"AzureSQL\").getOrCreate()\n",
    "\n",
    "## Credenciales BD\n",
    "inpath = \"C:/Users/diego.torres/OneDrive/Datasets/Tuboleta/Credenciales.txt\"\n",
    "keys = pd.read_csv(inpath, sep = ',')\n",
    "display(keys)\n",
    "\n",
    "# Creo variables para cada fila del DataFrame que contiene las credenciales de la bd\n",
    "for index, row in keys.iterrows():\n",
    "    variable_name = row['key']\n",
    "    variable_value = row['value']\n",
    "    globals()[variable_name] = variable_value\n",
    "\n",
    "# Configurar las propiedades de la conexi√≥n\n",
    "jdbc_url = f\"jdbc:sqlserver://{jdbc_hostname}:1433;database={jdbc_database_datamart}\"\n",
    "jdbc_properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": driver\n",
    "}\n",
    "\n",
    "# Conexion al Blob\n",
    "spark.conf.set(clave_blob, access_key_blob)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
