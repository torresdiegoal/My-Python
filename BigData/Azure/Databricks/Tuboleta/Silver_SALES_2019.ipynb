{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c19828-eeda-4807-b3ba-a8ae6e406581",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SALES: Silver 1 y 2\n",
    "Este notebook contiene el código para leer las tablas bronce, hacer la seleccion de variables, unir por año e imprimir esas bases como parquet.\n",
    "\n",
    "## Objetivos\n",
    "Los objetivos de este notebook son:\n",
    "\n",
    "* Lectura de tablas bronce azuresql\n",
    "* Selección de variables a trabajar en cada mes\n",
    "* Union de cada mes para formar una tabla por año\n",
    "* Imprimir en parquet por año\n",
    "\n",
    "## Creacion \n",
    "* Autor: Kevin Serrano\n",
    "* Fecha creación: 10/11/2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08f729eb-e562-4f3d-82d7-85cf8364cf18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Librerias y conexiones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28260778-fe38-419b-81c0-f650604d0978",
     "showTitle": true,
     "title": "Librerias"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "421c8ec6-39c9-46f3-863c-4f16cf380c34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Crear una SparkSession\n",
    "spark = SparkSession.builder.appName(\"AzureSQL\").getOrCreate()\n",
    "\n",
    "## Credenciales BD\n",
    "inpath = \"C:/Users/diego.torres/OneDrive/Datasets/Tuboleta/Credenciales.txt\"\n",
    "keys = pd.read_csv(inpath, sep = ',')\n",
    "display(keys)\n",
    "\n",
    "# Creo variables para cada fila del DataFrame que contiene las credenciales de la bd\n",
    "for index, row in keys.iterrows():\n",
    "    variable_name = row['key']\n",
    "    variable_value = row['value']\n",
    "    globals()[variable_name] = variable_value\n",
    "\n",
    "# Configurar las propiedades de la conexión\n",
    "jdbc_url = f\"jdbc:sqlserver://{jdbc_hostname}:1433;database={jdbc_database_datamart}\"\n",
    "jdbc_properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": driver\n",
    "}\n",
    "\n",
    "# Conexion al Blob\n",
    "spark.conf.set(clave_blob, access_key_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "576aa58f-be2c-4438-b5cf-8caa91427eae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Lectura de bases bronce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197d62c1-fe67-4f3d-94ee-b5b2102cfa34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Años ####\n",
    "# 2019 #\n",
    "Ds_B_Sales_2019_1=\"dbo.Ds_B_Sales_2019_1\"\n",
    "Ds_B_Sales_2019_2=\"dbo.Ds_B_Sales_2019_2\"\n",
    "Ds_B_Sales_2019_3=\"dbo.Ds_B_Sales_2019_3\"\n",
    "Ds_B_Sales_2019_4=\"dbo.Ds_B_Sales_2019_4\"\n",
    "Ds_B_Sales_2019_5=\"dbo.Ds_B_Sales_2019_5\"\n",
    "Ds_B_Sales_2019_6=\"dbo.Ds_B_Sales_2019_6\"\n",
    "Ds_B_Sales_2019_7=\"dbo.Ds_B_Sales_2019_7\"\n",
    "Ds_B_Sales_2019_8=\"dbo.Ds_B_Sales_2019_8\"\n",
    "Ds_B_Sales_2019_9=\"dbo.Ds_B_Sales_2019_9\"\n",
    "Ds_B_Sales_2019_10=\"dbo.Ds_B_Sales_2019_10\"\n",
    "Ds_B_Sales_2019_11=\"dbo.Ds_B_Sales_2019_11\"\n",
    "Ds_B_Sales_2019_12=\"dbo.Ds_B_Sales_2019_12\"\n",
    "\n",
    "## Lectura ##\n",
    "# 2019 #\n",
    "Ds_B_Sales_2019_1_df = spark.read.jdbc(url=jdbc_url, table=Ds_B_Sales_2019_1, properties=jdbc_properties)\n",
    "Ds_B_Sales_2019_2_df = spark.read.jdbc(url=jdbc_url, table=Ds_B_Sales_2019_2, properties=jdbc_properties)\n",
    "Ds_B_Sales_2019_3_df = spark.read.jdbc(url=jdbc_url, table=Ds_B_Sales_2019_3, properties=jdbc_properties)\n",
    "Ds_B_Sales_2019_4_df = spark.read.jdbc(url=jdbc_url, table=Ds_B_Sales_2019_4, properties=jdbc_properties)\n",
    "Ds_B_Sales_2019_5_df = spark.read.jdbc(url=jdbc_url, table=Ds_B_Sales_2019_5, properties=jdbc_properties)\n",
    "Ds_B_Sales_2019_6_df = spark.read.jdbc(url=jdbc_url, table=Ds_B_Sales_2019_6, properties=jdbc_properties)\n",
    "Ds_B_Sales_2019_7_df = spark.read.jdbc(url=jdbc_url, table=Ds_B_Sales_2019_7, properties=jdbc_properties)\n",
    "Ds_B_Sales_2019_8_df = spark.read.jdbc(url=jdbc_url, table=Ds_B_Sales_2019_8, properties=jdbc_properties)\n",
    "Ds_B_Sales_2019_9_df = spark.read.jdbc(url=jdbc_url, table=Ds_B_Sales_2019_9, properties=jdbc_properties)\n",
    "Ds_B_Sales_2019_10_df = spark.read.jdbc(url=jdbc_url, table=Ds_B_Sales_2019_10, properties=jdbc_properties)\n",
    "Ds_B_Sales_2019_11_df = spark.read.jdbc(url=jdbc_url, table=Ds_B_Sales_2019_11, properties=jdbc_properties)\n",
    "Ds_B_Sales_2019_12_df = spark.read.jdbc(url=jdbc_url, table=Ds_B_Sales_2019_12, properties=jdbc_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af5ef0f4-d4c1-4beb-9ec5-c5e4220f2918",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Silver 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d7d6f2-0c60-43b9-97dc-3eb6e10b0b43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Seleccion de columnas ##\n",
    "\n",
    "columnas = [\"ORGANIZATION\", \"T_ORGANIZ_ID\",\"PRODUCT_FAMILY\",\"PRODUCT\",\"INVOICE_CONTACT_FMT_NAME\",\"T_PRODUCT_ID\",\"LICENCE_NUMBER\",\"PRODUCT_STATE\",\"SITE\",\"SEASON\",\"TOPIC\",\"TOWN\",\"SUB_TOPIC\",\n",
    "\"REFERENCE_DATE\",\"ORDER_DATE_MM_YY\",\"PRODUCT_DATE_TIME\",\"PRODUCT_TIME\",\"FILE_NUMBER\",\"T_ORDER_CONTACT_ID\",\"PROMOTION\",\"AUDIENCE_CATEGORY\",\"AUDIENCE_SUB_CATEGORY\",\"LOGICAL_SEAT_CATEGORY\",\"PERFORMANCE_QUOTA\",\"SALES_CHANNEL\",\"SALES_CHANNEL_TYPE\",\"UNIT_AMT_ITX\",\"NET_SOLD_T_QTY\",\"NET_SOLD_C_QTY\",\"NET_SOLD_TKT_AMT_ITX\",\"TEAM_1\",\"TEAM_2\",\"ORDER_NUMBER\",\"CHARGE_AMT_ITX\", \"ORDER_CREATION_DATE\", \"ORDER_CONTACT_NUMBER\", \"ORDER_DATE\", \"PRODUCT_DATE\",\"CHARGE_AMT_ETX\", \"total_unit_amt_itx\",  \"DN_QUOTA\", \"T_INVOICE_CONTACT_ID\", \"File_state\", \"t_file_state\", \"ORDER_QUANTITY\", \"T_OPERATION_TYPE\", \n",
    "\"PRODUCT_CODE\",\"t_operation_kind\", \"parent_product\", \"INSURED\", \"INSURED_AMT_ITX\",\"T_PERFORMANCE_ID\",\"T_SITE_ID\",\"NET_SOLD_AMT_ITX\",\"PRODUCT_EXTERNAL_NAME\",\"T_SEASON_ID\",\"ACTIVITY\",\"T_ACTIVITY_ID\",\"T_PRODUCT_FAMILY_ID\",\"PRODUCT_TYPE\",\"T_PRODUCT_TYPE_ID\",\"T_FAMILY_TYPE\",\"T_FAMILY_SUB_TYPE\",\"T_TOPIC_ID\",\"T_SHIPMENTFEE_ID\",\"INSURED_AMT_ETX\",\"T_OPERATION_ID\",\"ORDER_CONTACT_ADDRESS\",\"ORDER_CONTACT_COUNTRY\",   \"ORDER_CONTACT_TOWN\",\"ORDER_CONTACT_PRIMARY_PHONE\",\"ORDER_CONTACT_EMAIL\"]\n",
    "\n",
    "### 2019 ###\n",
    "Ds_S1_Sales_2019_1=Ds_B_Sales_2019_1_df.select(columnas)\n",
    "Ds_S1_Sales_2019_2=Ds_B_Sales_2019_2_df.select(columnas)\n",
    "Ds_S1_Sales_2019_3=Ds_B_Sales_2019_3_df.select(columnas)\n",
    "Ds_S1_Sales_2019_4=Ds_B_Sales_2019_4_df.select(columnas)\n",
    "Ds_S1_Sales_2019_5=Ds_B_Sales_2019_5_df.select(columnas)\n",
    "Ds_S1_Sales_2019_6=Ds_B_Sales_2019_6_df.select(columnas)\n",
    "Ds_S1_Sales_2019_7=Ds_B_Sales_2019_7_df.select(columnas)\n",
    "Ds_S1_Sales_2019_8=Ds_B_Sales_2019_8_df.select(columnas)\n",
    "Ds_S1_Sales_2019_9=Ds_B_Sales_2019_9_df.select(columnas)\n",
    "Ds_S1_Sales_2019_10=Ds_B_Sales_2019_10_df.select(columnas)\n",
    "Ds_S1_Sales_2019_11=Ds_B_Sales_2019_11_df.select(columnas)\n",
    "Ds_S1_Sales_2019_12=Ds_B_Sales_2019_12_df.select(columnas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2698cab6-23de-40ab-9cd7-f5534cdcd8f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Silver 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55310bd8-661a-46ee-96e1-faec4c4b41a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Union de años ##\n",
    "\n",
    "# 2019 #\n",
    "Ds_S3_Sales_2019=Ds_S1_Sales_2019_1.union(Ds_S1_Sales_2019_2).union(Ds_S1_Sales_2019_3).union(Ds_S1_Sales_2019_4).union(Ds_S1_Sales_2019_5).union(Ds_S1_Sales_2019_6).union(Ds_S1_Sales_2019_7).union(Ds_S1_Sales_2019_8).union(Ds_S1_Sales_2019_9).union(Ds_S1_Sales_2019_10).union(Ds_S1_Sales_2019_11).union(Ds_S1_Sales_2019_12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd20467-a8b4-47f6-b5ad-e9f3947ec898",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Ajuste Canal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e84545-04fd-484f-9d2f-46f03a3231b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## canales de venta\n",
    "inpath = \"abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/Canales/Maestros/Canal.csv/\"\n",
    "canales = spark.read.csv(inpath,header=True, inferSchema=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41cf3512-9137-4218-9f42-d4440f6b767b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper, when, col\n",
    "# Realiza la unión basada en las columnas \"concatenated_column\" y \"CONCATENADO\"\n",
    "joined_df = Ds_S3_Sales_2019.join(canales, upper(Ds_S3_Sales_2019[\"SALES_CHANNEL\"]) == upper(canales[\"Canal de Venta\"]), how=\"left\")\n",
    "# Replace the joined column with the original value if the join didn't match\n",
    "joined_df = joined_df.withColumn(\"Canal\", when(col(\"Canal de Venta\").isNull(), col(\"SALES_CHANNEL\")).otherwise(col(\"Canal\")))\n",
    "\n",
    "# Drop the \"Canal de Venta\" column since we don't need it anymore\n",
    "Ds_S3_Sales_2019 = joined_df.drop(\"Canal de Venta\")\n",
    "\n",
    "# Renamed\n",
    "Ds_S3_Sales_2019 = Ds_S3_Sales_2019.withColumnRenamed('Canal','SALES_CHANNEL_N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "082fdaae-f26a-4bf3-85e0-c44fb6f4bd12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Ajuste TOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b620aa0-e21c-4ae5-8590-2aafa6819e74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## ciudad\n",
    "blob_storage_path = \"abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/Ciudades/Ciudades.csv/\"\n",
    "contacts = spark.read.csv(blob_storage_path,header=True, inferSchema=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "addd204d-1ffd-4d16-98be-da003ff633a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Columnas a concatenar\n",
    "columns_to_concat = [\"ORGANIZATION\", \"TOWN\", \"SITE\"]\n",
    "\n",
    "# Utiliza la función concat_ws para concatenar las columnas con un separador (en este caso, \"\")\n",
    "Ds_S3_Sales_2019_Town = Ds_S3_Sales_2019.withColumn(\"concatenated_column\", concat_ws(\"\", *columns_to_concat))\n",
    "\n",
    "\n",
    "# Realiza la unión basada en las columnas \"concatenated_column\" y \"CONCATENADO\"\n",
    "joined_df = Ds_S3_Sales_2019_Town.join(contacts, Ds_S3_Sales_2019_Town[\"concatenated_column\"] == contacts[\"CONCATENADO\"], \"left_outer\")\n",
    "\n",
    "# Borras las columnaas del csv\n",
    "columns_to_drop = [\"ORGANIZATION2\", \"TOWN2\", \"SITE2\", \"CONCATENADO\",\"concatenated_column\"]\n",
    "\n",
    "# Crea un nuevo DataFrame sin las columnas especificadas\n",
    "result_df = joined_df.drop(*columns_to_drop)\n",
    "\n",
    "#result_df = result_df.drop(\"TOWN\")\n",
    "# Renombra la columna \"HOMOLOGA\" como \"TOWN\"\n",
    "Ds_S3_Sales_2019 = result_df.withColumnRenamed(\"HOMOLOGA\", \"TOWN_N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f21950f-751c-4fe2-8292-5f92f3d41852",
     "showTitle": true,
     "title": "Ajuste TOPIC & SUBTOPIC"
    }
   },
   "outputs": [],
   "source": [
    "# Utiliza la función concat_ws para concatenar las columnas con un separador (en este caso, \"|\")\n",
    "Ds_S3_Sales_2019 = Ds_S3_Sales_2019.withColumn(\"TOPIC_KEY\", concat_ws(\"|\", *[\"TOPIC\", \"SUB_TOPIC\"]))\n",
    "\n",
    "## Maestra Topics\n",
    "inpath = \"abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/Modelos_analitica/Maestra_subtopic/\"\n",
    "subtopics = spark.read.csv(inpath + 'Maestra_subtopic.csv', sep = ';', header=True, inferSchema=True)\n",
    "#print(subtopics.count())\n",
    "#codpost = codpost.toPandas()\n",
    "\n",
    "subtopics.createOrReplaceTempView(\"subtopics_vw\")\n",
    "Ds_S3_Sales_2019.createOrReplaceTempView(\"Ds_S3_Sales_2019_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9853c946-0860-48ba-82ed-bb3cacba8fd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales = spark.sql(\"\"\"\n",
    "    select\n",
    "\t  s.*,\n",
    "      CASE WHEN t.TOPIC_N IS NULL \n",
    "            THEN 'SIN DATO'\n",
    "            ELSE t.TOPIC_N\n",
    "        END AS TOPIC_N,\n",
    "      CASE WHEN t.SUB_TOPIC_N_ IS NULL \n",
    "            THEN 'SIN DATO'\n",
    "            ELSE t.SUB_TOPIC_N_\n",
    "        END AS SUB_TOPIC_N\n",
    "\t  from Ds_S3_Sales_2019_vw s\n",
    "    left join subtopics_vw t\n",
    "      on s.TOPIC_KEY = t.TOPIC_KEY\n",
    "    order by PRODUCT,TOPIC_KEY\n",
    "\"\"\")\n",
    "\n",
    "#print(sales.count())\n",
    "#display(sales.limit(100))\n",
    "sales.createOrReplaceTempView(\"sales_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a1efa48-d0dd-4ce3-b1c9-7c6495169e51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Maestra_subtopic_2\n",
    "inpath = 'abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/Modelos_analitica/Maestra_subtopic/maestra_topic_2.csv/'\n",
    "Maestra_subtopic_2 = spark.read.csv(inpath,header=True, inferSchema=True, sep=';')\n",
    "#print(Maestra_subtopic_2.count())\n",
    "Maestra_subtopic_2.createOrReplaceTempView(\"Maestra_subtopic_2_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ded8df70-69f9-45f4-adad-46b0f121bfd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales = spark.sql(\"\"\"\n",
    "    --el numero de eventos se contabiliza aparte debido a que para llegar a el, se deben realizar filtros distintos \n",
    "    --with num_eventos_ as (\n",
    "\tselect \n",
    "\t  s.*,\n",
    "\t  CASE WHEN m.NUEVO_TOPIC IS NULL\n",
    " \t\t\tTHEN s.TOPIC_N\n",
    "            ELSE m.NUEVO_TOPIC END AS TOPIC_NW, \n",
    "\t  CASE WHEN m.NUEVO_SUBTOPIC IS NULL\n",
    " \t\t\tTHEN s.SUB_TOPIC_N\n",
    "            ELSE m.NUEVO_SUBTOPIC END AS SUB_TOPIC_NW\n",
    "\tfrom sales_vw s\n",
    "\tleft join Maestra_subtopic_2_vw m\n",
    "\t\ton s.PRODUCT = m.PRODUCT\n",
    "\t\"\"\")\n",
    "\n",
    "#print(Ds_S3_Sales_2022_2023_2024.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8de6a3bd-4316-41f3-9936-1754cd94b7b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Impresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b7efa0-cb11-4c65-b1bb-c9670a94f7f7",
     "showTitle": true,
     "title": "2019"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guarda el DataFrame en formato Parquet en Azure Blob Storage\n",
    "inpath = \"abfss://storagebi@tbdwhstorage01.dfs.core.windows.net/silver/sales/2019/\"\n",
    "\n",
    "# 2019_final\n",
    "join_df_coalesced = sales.coalesce(1)\n",
    "join_df_coalesced.write.mode(\"overwrite\").parquet(inpath)\n",
    "\n",
    "#cambio de nombre\n",
    "files=dbutils.fs.ls(inpath)\n",
    "output_file= [x for x in files if x.name.startswith(\"part-\")]\n",
    "# En la misma carpeta de origen\n",
    "dbutils.fs.mv(output_file[0].path, f\"{inpath}/Ds_S3_Sales_2019.parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SALES_HIST_2019: Silver 1-3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
